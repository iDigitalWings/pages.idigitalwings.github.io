import{_ as e,a,af as o,o as n}from"./chunks/framework.C87LdZyP.js";const r="/assets/2708367421216708.txfbx1ZQ.png",p="/assets/2712912232497125.CKQhpy3v.png",s="/assets/2713245144910916.BG6MhmrE.png",u=JSON.parse('{"title":"PromptEngineering：自洽性（Self Consistency）改善思维链在语言模型中的推理","description":"","frontmatter":{"title":"PromptEngineering：自洽性（Self Consistency）改善思维链在语言模型中的推理","date":"2023-07-10T00:00:00.000Z","tags":["ai","ml"],"category":["AI"]},"headers":[],"relativePath":"topics/prompt-engineering/articles/2023-07-10-Prompt-Engineering-self-consistency.md","filePath":"topics/prompt-engineering/articles/2023-07-10-Prompt-Engineering-self-consistency.md","lastUpdated":1718173059000}'),i={name:"topics/prompt-engineering/articles/2023-07-10-Prompt-Engineering-self-consistency.md"};function c(l,t,g,d,m,h){return n(),a("div",null,t[0]||(t[0]=[o('<div class="admonition abstract"><p class="admonition-title">abstract</p><p>今天是提示工程（Prompt Engineering）的第三篇，介绍 <strong>自洽性（Self Consistency）</strong>。 自洽性是一种建立在 <strong>思维链(CoT)</strong> 提示基础上的高级提示技术。 他是通过采样多个不同的推理路径并选择最一致的答案来改进使用 CoT 提示的朴素贪婪解码。</p></div><p>简单点儿说就是说，你问 AI 一个问题，他让5个人一起作答，每个人的解题思路不同， 当然算出来的答案也可能不同，最终少数服从多数，采取最通用的那个答案。</p><p><img src="'+r+'" alt="自洽性提示技术的原理"></p><h2 id="关于-cot" tabindex="-1">关于 CoT <a class="header-anchor" href="#关于-cot" aria-label="Permalink to &quot;关于 CoT&quot;">​</a></h2><p>前面首先提到，自洽性是改善了思维链，思维链（CoT）是什么，大家可以读我的另一篇文章：todo</p><h2 id="论文" tabindex="-1">论文 <a class="header-anchor" href="#论文" aria-label="Permalink to &quot;论文&quot;">​</a></h2><p>自洽性在去年（2022年）被提出，并且作为会议论文发表在 ICLR 2023 上。</p><p>大家有兴趣可以读一下论文原文：<a href="https://arxiv.org/pdf/2203.11171.pdf" target="_blank" rel="noreferrer">《自洽性改善了思维链在语言模型的推理》</a>。</p><p>论文摘要：</p><blockquote><p>自洽性以取代思想链提⽰中使⽤的朴素贪婪解码。它⾸先对⼀组不同的推理路径进⾏采样，⽽不是只采⽤贪婪的推理路径，然后通过边缘化采样的推理路径来选择最⼀致的答案。</p></blockquote><blockquote><p>自洽性利用了一种直觉，即复杂的推理问题通常会采用多种不同的思维方式来得出其独特的正确答案。</p></blockquote><blockquote><p>⼈性的⼀个显着特点是⼈们的思维⽅式不同。</p></blockquote><blockquote><p>很⾃然地假设在任务中需要深思熟虑，可能有⼏种⽅法可以解决这个问题。可以通过从语⾔模型的解码器采样来在语⾔模型中模拟这样的过程。</p></blockquote><h2 id="关于示例" tabindex="-1">关于示例 <a class="header-anchor" href="#关于示例" aria-label="Permalink to &quot;关于示例&quot;">​</a></h2><p>上面介绍的自洽性提示，但是前面我并没有像上一篇一样给出 ChatGPT 示例的示例，有两个原因。</p><p>首先当前的 LLM 变得越来越聪明了，就像论文中提到的，语言模型的解码器可以模拟实现这样的过程， 这就导致我们根本看不到刚才提到的自洽性推理选择过程，因为 LLM 直接返回了一个最终的答案，以及最合适的推理过程给我们。</p><p>同时经过不断的训练，一些开源数据集也已经可以被准确的预测，拿这些测试没有太多意义。</p><p><img src="'+p+'" alt="ChatGPT 返回了最佳答案及推理过程"></p><p>其次，如果对于复杂问题，语言模型编码器凭借 <strong>直觉</strong> 没有帮你做自洽性推理，那么你需要进行少样本的提示， 引导模型进行自动或者手动进行多次推理，这样在你不知道正确答案的时候并不太容易发生。对一般人实用性并不强。</p><p>以下是作者在论文中对于不同数据集和模型上使用思维链提示 和 自洽性提示对效果提升的一个表格。 <img src="'+s+'" alt="思维链提⽰和自洽性的算术推理准确性对比"></p><h2 id="成本" tabindex="-1">成本 <a class="header-anchor" href="#成本" aria-label="Permalink to &quot;成本&quot;">​</a></h2><p>很明显，自洽性提示需要先进行多次推理，然后再选取最优答案，也造成了他一个明显的局限性：产⽣更多的计算成本。</p><p>针对这个问题，有两个方法可以缓解。</p><p>第一就是使用少量的推理路径，作者建议是5到10个， 一般情况下这个数量已经可以达到性能饱和。</p><p>第二就是使用自洽性提示来进行模型微调，这样微调之后的模型可以在单次推理运⾏中给出更准确的预测。</p><h2 id="结语" tabindex="-1">结语 <a class="header-anchor" href="#结语" aria-label="Permalink to &quot;结语&quot;">​</a></h2><p>虽然自洽性提示技术并不会在我们直接和 AI 对话的时候使用， 但是你在使用甚至自己编写 AI 工具的时候，自洽性提示方法和思想还是能发挥很大作用。</p>',27)]))}const b=e(i,[["render",c]]);export{u as __pageData,b as default};
