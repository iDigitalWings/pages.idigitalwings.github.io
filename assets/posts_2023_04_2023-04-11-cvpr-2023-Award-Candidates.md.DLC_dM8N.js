import{_ as t,a as e,af as i,o}from"./chunks/framework.C87LdZyP.js";const r="/assets/83296881166291.72j4SthM.gif",s="/assets/84410204476375.DLmQuBBy.png",l="/assets/84480225532166.0cmhrMNP.png",n="/assets/85703685474125.OPllxmEC.png",p="/assets/85849334792250.Db6d13Tw.png",c="/assets/86365094672250.CB8bPOBQ.png",f=JSON.parse('{"title":"CVPR 2023 候选获奖论文都讲了什么(1/4)","description":"","frontmatter":{"title":"CVPR 2023 候选获奖论文都讲了什么(1/4)","date":"2023-04-11T00:00:00.000Z","tags":["ai","ml"],"category":["AI"]},"headers":[],"relativePath":"posts/2023/04/2023-04-11-cvpr-2023-Award-Candidates.md","filePath":"posts/2023/04/2023-04-11-cvpr-2023-Award-Candidates.md","lastUpdated":1718173059000}'),h={name:"posts/2023/04/2023-04-11-cvpr-2023-Award-Candidates.md"};function d(g,a,m,u,b,D){return o(),e("div",null,a[0]||(a[0]=[i('<blockquote><p>本文给大家快速解读一下 CVPR 2023 候选获奖论文都讲了什么。 周二6篇，周四六篇，总共12篇论文，内容较多， 我会分四期分别介绍，今天介绍周二的前三篇。</p></blockquote><h2 id="ego-body-pose-estimation-via-ego-head-pose-estimation" tabindex="-1">Ego-Body Pose Estimation via Ego-Head Pose Estimation <a class="header-anchor" href="#ego-body-pose-estimation-via-ego-head-pose-estimation" aria-label="Permalink to &quot;Ego-Body Pose Estimation via Ego-Head Pose Estimation&quot;">​</a></h2><p><strong>《通过自我头部姿势估计进行自我身体姿势估计》</strong></p><p><img src="'+r+'" alt="EgoEgo将以自我为中心的视频作为输入来预测头部姿势和估计全身姿势"></p><p><strong>一句话</strong>：拿着一个你用 VR 眼镜或者以头部为中心拍的视频（类似人眼看到的），来帮你构建出人体的全身姿势。</p><h3 id="论文摘要" tabindex="-1">论文摘要 <a class="header-anchor" href="#论文摘要" aria-label="Permalink to &quot;论文摘要&quot;">​</a></h3><p>从以自我为中心的视频序列中估计 3D 人体运动对于 VR/AR 中的人类行为理解和应用至关重要。然而，天真地学习以自我为中心的视频和人体动作之间的映射是具有挑战性的，因为放置在用户头部的前置摄像头通常无法观察到用户的身体。此外，通过配对的自我中心视频和 3D 人体动作收集大规模、高质量的数据集需要精确的动作捕捉设备，这通常将视频中的场景种类限制在类似实验室的环境中。 为了消除配对自我中心视频和人体运动的需要，我们提出了一种新方法，通过自我头部姿势估计进行自我身体姿势估计（EgoEgo），该方法将问题分解为两个阶段，通过头部运动作为中间表示连接。EgoEgo 首先集成了 SLAM 和学习方法来估计准确的头部运动。然后，将估计的头部姿势作为输入，它利用条件扩散来生成多个似是而非的全身运动。这种头部和身体姿势的分离消除了使用配对的自我中心视频和 3D 人体运动训练数据集的需要，使我们能够分别利用大规模自我中心视频数据集和动作捕捉数据集。 此外，为了进行系统基准测试，我们开发了一个合成数据集 AMASS-Replica-Ego-Syn (ARES)，其中包含配对的自我中心视频和人体运动。在 ARES 和真实数据上，我们的 EgoEgo 模型都比最先进的模型表现得更好。</p><h3 id="论文地址" tabindex="-1">论文地址 <a class="header-anchor" href="#论文地址" aria-label="Permalink to &quot;论文地址&quot;">​</a></h3><p>论文网站：</p><ul><li><a href="https://lijiaman.github.io/projects/egoego/" target="_blank" rel="noreferrer">https://lijiaman.github.io/projects/egoego/</a></li></ul><p>论文下载：</p><ul><li><a href="https://arxiv.org/pdf/2212.04636.pdf" target="_blank" rel="noreferrer">https://arxiv.org/pdf/2212.04636.pdf</a></li></ul><h2 id="《3d-registration-with-maximal-cliques》" tabindex="-1">《3D Registration with Maximal Cliques》 <a class="header-anchor" href="#《3d-registration-with-maximal-cliques》" aria-label="Permalink to &quot;《3D Registration with Maximal Cliques》&quot;">​</a></h2><p>《使用极大团进行 3D 配准》</p><p><strong>一句话</strong>：这篇论文阐述了使用极大团的方案来解决3D 配准的问题，</p><p>先解释下几个概念：</p><h3 id="团-cliques" tabindex="-1">团（Cliques） <a class="header-anchor" href="#团-cliques" aria-label="Permalink to &quot;团（Cliques）&quot;">​</a></h3><p><strong>团（Cliques）</strong> 是一个无向图的完全子图，而完全图，当然每对顶点之间都必须要有边相连。</p><p><img src="'+s+'" alt="团（Cliques）示例"></p><p>下图是一个经典的示例：</p><h3 id="极大团-maximal-cliques" tabindex="-1">极大团（Maximal Cliques） <a class="header-anchor" href="#极大团-maximal-cliques" aria-label="Permalink to &quot;极大团（Maximal Cliques）&quot;">​</a></h3><p><strong>极大团（Maximal Cliques）</strong>：如果一个团不被其他任一团所包含，即它不是其他任一团的真子集，则称该团为图G的极大团</p><p><img src="'+l+'" alt=""></p><h3 id="_3d-配准-3d-registration" tabindex="-1">3D 配准（3D Registration） <a class="header-anchor" href="#_3d-配准-3d-registration" aria-label="Permalink to &quot;3D 配准（3D Registration）&quot;">​</a></h3><p><strong>3D 配准（3D Registration）</strong>：AR 应用程序可以获得参考空间框架来放置虚拟对象，使其与真实对象的预期位置相匹配的过程。</p><p><img src="'+n+'" alt="使用3D配准，三维物体形状可以通过 3D 传感器捕获的面来确定"></p><p>再来看论文，当然你可能还是看不懂，不过你能大概知道论文是干啥的了。</p><p><img src="'+p+'" alt=""></p><h3 id="论文摘要-1" tabindex="-1">论文摘要 <a class="header-anchor" href="#论文摘要-1" aria-label="Permalink to &quot;论文摘要&quot;">​</a></h3><p>我们提出了一种具有最大团 (MAC) 的 3D 配准方法。关键的见解是放松先前的最大派系约束， 并在图中挖掘更多局部共识信息以生成准确的姿势假设：</p><ol><li>构建兼容性图以呈现初始对应关系之间的亲和关系。</li><li>我们在图中搜索最大派系，每个派系代表一个共识集。然后我们执行节点引导的团选择，其中每个节点对应于具有最大图权重的最大团。</li><li>通过SVD算法为选定的团计算变换假设，并使用最佳假设进行注册。</li></ol><p>在 U3M、3DMatch、3DLoMatch 和 KITTI 上的大量实验表明 MAC 有效地提高了配准精度， 优于各种最先进的方法，并提高了深度学习方法的性能。MAC 与深度学习方法相结合， 实现了最先进的注册召回 3DMatch / 3DLoMatch 上为95.7% / 78.9%。</p><h3 id="论文地址-1" tabindex="-1">论文地址 <a class="header-anchor" href="#论文地址-1" aria-label="Permalink to &quot;论文地址&quot;">​</a></h3><p>项目地址：</p><ul><li><a href="https://github.com/zhangxy0517/3D-Registration-with-Maximal-Cliques" target="_blank" rel="noreferrer">https://github.com/zhangxy0517/3D-Registration-with-Maximal-Cliques</a></li></ul><h2 id="《omniobject3d-large-vocabulary-3d-object-dataset-for-realistic-perception-reconstruction-and-generation》" tabindex="-1">《OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation》 <a class="header-anchor" href="#《omniobject3d-large-vocabulary-3d-object-dataset-for-realistic-perception-reconstruction-and-generation》" aria-label="Permalink to &quot;《OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation》&quot;">​</a></h2><p>《OmniObject3D：用于真实感知、重建和生成的大词汇量 3D 对象数据集》 这个项目由来自 上海人工智能实验室， 香港中文大学， 商汤科技，香港科技大学， 南洋理工大学S-Lab 的成员共同完成。</p><p><strong>一句话</strong>：<strong>OmniObject3D</strong>，是一个包含大量高质量真实扫描 3D 对象的大词汇量 3D 对象数据集，可以进行视图生成、表面重建、对象生成等。</p><p><img src="'+c+'" alt=""></p><h3 id="项目特性" tabindex="-1">项目特性 <a class="header-anchor" href="#项目特性" aria-label="Permalink to &quot;项目特性&quot;">​</a></h3><p>OmniObject3D 有几个吸引人的特性：</p><ol><li>大词汇量：它包含190 个日常类别中的 6,000 个扫描对象，与流行的 2D 数据集（例如 ImageNet 和 LVIS）共享通用类，有利于追求可概括的 3D 表示。</li><li>Rich Annotations：每个 3D 对象都使用 2D 和 3D 传感器捕获，提供纹理网格、点云、多视图渲染图像和多个实拍视频。</li><li>现实扫描：专业的扫描仪支持高质量的物体扫描，具有精确的形状和逼真的外观。</li></ol><p>借助 OmniObject3D 提供的广阔探索空间，我们精心设置了四个评估轨道：</p><ol><li>稳健的 3D 感知，</li><li>新视图合成，</li><li>神经表面重建，以及</li><li>3D 对象生成。</li></ol><h3 id="地址" tabindex="-1">地址 <a class="header-anchor" href="#地址" aria-label="Permalink to &quot;地址&quot;">​</a></h3><p>项目地址：</p><ul><li><a href="https://omniobject3d.github.io/" target="_blank" rel="noreferrer">https://omniobject3d.github.io/</a></li></ul>',47)]))}const x=t(h,[["render",d]]);export{f as __pageData,x as default};
