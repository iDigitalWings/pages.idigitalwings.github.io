import{_ as a,a as i,af as t,o as p}from"./chunks/framework.C87LdZyP.js";const e="/assets/123104525191000.DH_jFnS0.png",l="/assets/3129010066958.ifssqpGs.png",n="/assets/4884717269791.jewyajuQ.png",o="/assets/56602231109500.DOKBdoQP.png",h="/assets/56816791400750.C6defwxq.png",d="/assets/52989001498958.ByawJuT0.png",r="/assets/53300633390125.CECYZTtc.png",c="/assets/53441882806833.Dp0AT-0V.png",g="/assets/53602023277791.BuBeO8mP.png",k="/assets/53651332333166.BoZdHlXv.png",m="/assets/53670038122416.Djk9c-00.png",u="/assets/54148895410791.D3r0Fi-G.png",F="/assets/54308582983166.CaE2PLG4.png",b="/assets/54435270971708.D-LsHYI0.png",y="/assets/54480467082375.C7r4srFn.png",_="/assets/54555343217250.B2EHaEZA.png",C="/assets/54707335080000.CZQI69hG.png",q="/assets/54864032267291.BfPed_Dx.png",v="/assets/32728872805750.1dXpHmC0.png",D="/assets/55013250092750.C2jUMGUl.png",L="/assets/55138717263791.DFaF0Tjr.png",S="/assets/57484658034125.DhjSvtyZ.png",f="/assets/57631535670375.Cg0vNTO2.png",M="/assets/58772625682958.BQzLq1ts.png",P="/assets/58888806678166.BheB--qY.png",x="/assets/58932961202083.BElYG8vE.png",A="/assets/59149673174833.GUnj_p2e.png",B="/assets/60671242808125.BmD3fECS.png",T=JSON.parse('{"title":"本地 LLM 可视化工具 LM Studio 突破国内网络限制使用","description":"","frontmatter":{"title":"本地 LLM 可视化工具 LM Studio 突破国内网络限制使用","date":"2024-07-25 10:00:00","tags":["llm-local-api"],"category":["AI"]},"headers":[],"relativePath":"posts/2024/07/2024-07-25-lm-studio.md","filePath":"posts/2024/07/2024-07-25-lm-studio.md","lastUpdated":1722239098000}'),I={name:"posts/2024/07/2024-07-25-lm-studio.md"};function G(E,s,U,w,O,H){return p(),i("div",null,s[0]||(s[0]=[t('<div class="admonition abstract"><p class="admonition-title">abstract</p><p>LM Studio 是一款本地运行大模型(LLM)的 GUI 程序，本文讲述如何配置 LM Studio 网络使其可以在国内下载和运行模型。</p></div><p>前面介绍了 Ollama 这个本地 LLM 工具，可以作为 OpenAPI 的本地替代方案， 不过其使用方式是基于命令行或者 API 的，如果我们只是想简单用一下，就显得不太方便。</p><p>或者说 Ollama 还是面向技术人员的，如果非技术人员来说，也有很多优秀的解决方法， 今天介绍的 LM Studio 就是不需要懂技术， 甚至不需要再终端输命令就可以本地用上大模型。</p><h2 id="介绍" tabindex="-1">介绍 <a class="header-anchor" href="#介绍" aria-label="Permalink to &quot;介绍&quot;">​</a></h2><p>LM Studio 的功能很简单，</p><ul><li>在笔记本电脑上运行 LLM，保证完全离线</li><li>通过应用内聊天 UI 或与 OpenAI 兼容的本地服务器使用模型</li><li>从 HuggingFace 🤗 存储库下载任何兼容的模型文件</li></ul><p>访问 LM Studio 的官网 <a href="https://lmstudio.ai/" target="_blank" rel="noreferrer">lmstudio.ai</a> 直接下载对应的版本，</p><p><img src="'+e+'" alt=""></p><p>下载安装双击打开应用，看到搜索框，推荐模型，功能列表。</p><p><img src="'+l+'" alt=""></p><h2 id="模型下载的问题" tabindex="-1">模型下载的问题 <a class="header-anchor" href="#模型下载的问题" aria-label="Permalink to &quot;模型下载的问题&quot;">​</a></h2><p>查看推荐的模型，然后挑一个下载。</p><p><img src="'+n+'" alt=""></p><p>以为一切顺利的时候，问题出现了。</p><p>所有的模型都在 HuggingFace 上，由于众所周知的网络问题， <code>huggingface.co</code> 无法访问。 这也导致了模型看得见，下载不了。</p><p>使用代理是一个好方法，但是：由于 LM Studio <strong>不支持 HTTP_PROXY</strong>，所以你的代理他也用不上。</p><h3 id="使用-huggingface-国内镜像" tabindex="-1">使用 HuggingFace 国内镜像 <a class="header-anchor" href="#使用-huggingface-国内镜像" aria-label="Permalink to &quot;使用 HuggingFace 国内镜像&quot;">​</a></h3><p>HuggingFace 国内有一个镜像站， <a href="https://hf-mirror.com/" target="_blank" rel="noreferrer">https://hf-mirror.com/</a> ，我们可以使用这个网站来下载模型。</p><p><img src="'+o+'" alt=""></p><p>不好的地方是， LM Studio 没有导入模型的功能，不然在浏览器上直接下载导入即可。</p><h3 id="macos" tabindex="-1">MacOS <a class="header-anchor" href="#macos" aria-label="Permalink to &quot;MacOS&quot;">​</a></h3><p>经过查找，发现首页出现的代理都是安装包文件里面写的，安装之后在本机形成一个 JSON 文件，地址如下（我的 MacOS 系统）：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">~</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">/Library/Application</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;">\\ </span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">Support/LM</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;">\\ </span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">Studio/model-catalog.json</span></span></code></pre></div><p>我们用编辑器打开，批量替换 <code>https://huggingface.co/</code> -&gt; <code>https://hf-mirror.com/</code> 。</p><p>该镜像可以在国内网络成功下载模型，</p><p><img src="'+h+'" alt="模型下载中"></p><p>用了一会儿发现一个问题，这个文件只是临时的，会被程序动态修改覆盖。</p><p>下面我们修改安装包里面的内容，彻底解决这个问题。</p><p>找到 <code>LM Studio.app</code> 然后右键打开文件内容：</p><p><img src="'+d+'" alt="查看 APP 内容"></p><p>用文件编辑器打开下面文件夹：</p><p><code>LM Studio.app/Contents/Resources/app/.webpack/main</code></p><p>比如我最喜欢的 Sublime 编辑器，</p><p><img src="'+r+'" alt="使用编辑器替换"></p><p>重新执行上面的查找替换，把所有的网址都替换掉，关闭重新打开 LM Studio 程序即可。</p><div class="admonition note"><p class="admonition-title">直接替换域名</p><p>如果大家替换网址的话，会发现打开模型卡片等功能还不能正常使用，需要直接替换域名：</p><p><code>huggingface.co</code> -&gt; <code>hf-mirror.com</code></p></div><p>现在我们再首页执行以下搜索，比如搜索阿里的 <code>qwen</code> 模型，可以看到可以搜索、查看模型、下载，都可以正常是使用了。</p><p><img src="'+c+'" alt="使用镜像站执行搜索和查看"></p><h3 id="windows-系统" tabindex="-1">Windows 系统 <a class="header-anchor" href="#windows-系统" aria-label="Permalink to &quot;Windows 系统&quot;">​</a></h3><p>Window 系统的处理方案也一样。</p><p>在 Windows 系统上安装 LM Studio 时不能指定目录，都会安装到下面目录：</p><ul><li><code>C:\\Users/[你 Windows 的用户名]\\AppData\\LM-Studio</code></li></ul><p>进入到这个目录之后可以看到你下载的 LM Studio 程序都包含哪些文件， 同样是打开文件夹（使用 Sublime、VS Code 等工具）， 然后执行之前说的替换：</p><ul><li><code>huggingface.co</code> 都替换成 <code>hf-mirror.com</code> 即可</li></ul><p>最后重启 LM Studio 就可以正常使用了。</p><h2 id="聊天" tabindex="-1">聊天 <a class="header-anchor" href="#聊天" aria-label="Permalink to &quot;聊天&quot;">​</a></h2><p>作为 GUI 程序，LM Studio 内置了聊天功能。</p><p>点击左侧功能列表的聊天图标，切换到聊天界面，并选择你期望期望使用的模型。我们选择刚刚下载的 「Llama 3.1」。</p><p><img src="'+g+'" alt="选择已经下载的模型"></p><p>经过短暂地加载进度条之后，</p><p><img src="'+k+'" alt="模型加载中"></p><p>模型亮起来就表示加载完毕了。</p><p><img src="'+m+'" alt="模型加载成功"></p><p>聊天界面左中右分别是对话列表、聊天窗口、设置面板。</p><p><img src="'+u+'" alt=""></p><p>左右两个面板都可以收起来，主要看下设置面板，设置面板可以自定义很多内容，常用的有：</p><ul><li>系统提示词、</li><li>上下文长度、</li><li>温度、</li><li>最长输出长度、</li><li>CPU线程数</li></ul><p><img src="'+F+'" alt="聊天设置面板"></p><p>当然还有一些别的，不过建议看不懂就别改了。</p><p><img src="'+b+'" alt=""></p><h2 id="playground" tabindex="-1">PlayGround <a class="header-anchor" href="#playground" aria-label="Permalink to &quot;PlayGround&quot;">​</a></h2><p><img src="'+y+'" alt=""></p><p>PlayGround 模块和 Ollama 启动多个模型类似，不过我们咱们电脑要是内存不多，就不建议尝试了，免得给电脑整死机。</p><p><img src="'+_+'" alt=""></p><h2 id="server" tabindex="-1">Server <a class="header-anchor" href="#server" aria-label="Permalink to &quot;Server&quot;">​</a></h2><p>LM Studio 当然也能作为服务运行，进入到 Server 页面，选择模型，然后点击启动服务即可。</p><p><img src="'+C+'" alt=""></p><p>启动服务之后，可以很方便的在日志的文本框里面查看日志，确实比 Ollama 方便多了。</p><p><img src="'+q+'" alt=""></p><h3 id="embedding" tabindex="-1">Embedding <a class="header-anchor" href="#embedding" aria-label="Permalink to &quot;Embedding&quot;">​</a></h3><p>如果用来开发 RAG 程序，嵌入是少不了的， LM Studio 也可以很方便的使用嵌入模型。</p><p>同样是在 Server 页面，点击中间「嵌入模型」面板的下载，就可以下载嵌入模型。</p><p><img src="'+v+'" alt=""></p><p>这里默认提供的是 <code>nomic embed text v1 5 Q8_0</code></p><p><img src="'+D+'" alt=""></p><h3 id="调用" tabindex="-1">调用 <a class="header-anchor" href="#调用" aria-label="Permalink to &quot;调用&quot;">​</a></h3><p>LM Studio 还有一个很任性的地方就是直接把调用代码放在界面上了。和谷歌的 AI Studio 一样，对开发人员很友好。</p><p><img src="'+L+'" alt="集成各种语言的代码示例"></p><p>我们用 CURL 测试一下，看到和 Ollama 接口默认一样使用了流式输出：</p><p><img src="'+S+`" alt="流式API返回"></p><p>我们设置 Stream 参数 为 false ，</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> http://localhost:1234/v1/chat/completions</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">  -H</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &quot;Content-Type: application/json&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">  -d</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;{ </span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    &quot;model&quot;: &quot;lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    &quot;messages&quot;: [ </span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">      { &quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Always answer in rhymes.&quot; },</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">      { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Introduce yourself.&quot; }</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    ], </span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    &quot;temperature&quot;: 0.7, </span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    &quot;max_tokens&quot;: -1,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    &quot;stream&quot;: false</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">}&#39;</span></span></code></pre></div><p>看到结果直接返回了：</p><p><img src="`+f+`" alt="直接返回结果"></p><h2 id="cli" tabindex="-1">CLI <a class="header-anchor" href="#cli" aria-label="Permalink to &quot;CLI&quot;">​</a></h2><p>作为客户端 GUI 工具，为了满足技术人员 CLI 的需求， 从 LM Studio 0.2.22 开始，官方发布了 LM Studio 的配套 CLI 工具。</p><p>您lms可以加载/卸载模型、启动/停止 API 服务器，并检查原始 LLM 输入（而不仅仅是输出）。</p><p><code>lms</code> 随 LM Studio 一起提供，位于 LM Studio 的工作目录中 <code>~/.cache/lm-studio/bin/</code>。 当您更新 LM Studio 时，它也会更新您的<code>lms</code>版本。</p><blockquote><p>需要至少运行一次 LM Studio 才可以使用 <code>lms</code>。</p></blockquote><p>我们可以使用一下命令启动 <code>lms</code> ：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># Mac / Linux:</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">~</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">/.cache/lm-studio/bin/lms bootstrap</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># Windows:</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">cmd</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> /c</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> %USERPROFILE%/.cache/lm-studio/bin/lms.exe</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> bootstrap</span></span></code></pre></div><p><img src="`+M+`" alt=""></p><p><code>lms</code>是 MIT 许可的，它是在 GitHub 上的这个存储库中开发的： <a href="https://github.com/lmstudio-ai/lms" target="_blank" rel="noreferrer">https://github.com/lmstudio-ai/lms</a></p><h3 id="lms-命令" tabindex="-1">lms 命令 <a class="header-anchor" href="#lms-命令" aria-label="Permalink to &quot;lms 命令&quot;">​</a></h3><p>启动和停止服务：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">lms</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> server</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> start</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">lms</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> server</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> stop</span></span></code></pre></div><p>列出模型：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">lms</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ls</span></span></code></pre></div><p><img src="`+P+'" alt=""></p><p>查看当前加载模型：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">lms</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ps</span></span></code></pre></div><p><img src="'+x+'" alt=""></p><p>加载模型（带选项）：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">lms</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> load</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> [--gpu=max</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">|</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">auto</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">|</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">0.0-1.0]</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> [--context-length=1-N]</span></span></code></pre></div><p><code>--gpu=1.0</code>意思是「尝试将 100% 的计算卸载到 GPU」。</p><p>还为您的本地 LLM 分配一个标识符：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">lms</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> load</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> TheBloke/phi-2-GGUF</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --identifier=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;gpt-4-turbo&quot;</span></span></code></pre></div><h3 id="调试提示" tabindex="-1">调试提示 <a class="header-anchor" href="#调试提示" aria-label="Permalink to &quot;调试提示&quot;">​</a></h3><p><code>lms log stream</code> 允许您检查进入模型的确切输入字符串。</p><p>这对于调试提示模板问题和其他意外的 LLM 行为特别有用。</p><p><img src="'+A+'" alt=""></p><p>不过既然我们用了 GUI， 我想一般很少有人会使用 <code>lms</code> 吧。</p><h2 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-label="Permalink to &quot;总结&quot;">​</a></h2><p>对于个人电脑来说，我觉得还是在线网页程序或者本地 GUI 程序使用起来更方便。 相比本地的 GUI 程序，我个人还是倾向在线的网页应用。</p><p><img src="'+B+'" alt=""></p><p>所以，LM Studio 这样的工具价值肯定不是让你在本地聊天的，因为聊天体验肯定比不上线上的应用， 无论是质量、速度，还是功能。</p><p>下篇文章和大家一起探究如何让本地大模型为普通人产生价值。</p>',117)]))}const R=a(I,[["render",G]]);export{T as __pageData,R as default};
