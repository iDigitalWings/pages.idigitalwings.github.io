import{_ as r,a as e,af as o,o as l}from"./chunks/framework.C87LdZyP.js";const a="/assets/797778558088541.ZZgqQ10I.webp",d="/assets/796840430881875.DnDgh58H.png",m=JSON.parse('{"title":"LMSYS 聊天机器人竞技排行榜 2023-05-03","description":"","frontmatter":{"title":"LMSYS 聊天机器人竞技排行榜 2023-05-03","date":"2023-03-19T00:00:00.000Z","tags":["ai","ml"],"category":["AI"]},"headers":[],"relativePath":"posts/2023/03/2023-03-19-chatbot-leaderboard-20230503.md","filePath":"posts/2023/03/2023-03-19-chatbot-leaderboard-20230503.md","lastUpdated":1718173059000}'),s={name:"posts/2023/03/2023-03-19-chatbot-leaderboard-20230503.md"};function n(h,t,i,p,b,c){return l(),e("div",null,t[0]||(t[0]=[o('<blockquote><p>为了促进LLM在聊天机器人领域的发展和创新， <strong>LMSYS Org</strong> 创建了一个名为 <strong>Chatbot Arena</strong> 的平台。 它展示了不同的聊天机器人模型在与真实用户对话中的性能和评分。 不同的机器人以众包方式提供匿名，随机的战斗， 该平台计划定期更新一次排行榜，根据模型与用户对话的结果来计算模型的Elo评分。</p></blockquote><p>首先先看下排名，再详细说：</p><p><img src="'+a+'" alt="流行的开源大型语言模型的 Elo 评级 230503"></p><h2 id="项目背景" tabindex="-1">项目背景 <a class="header-anchor" href="#项目背景" aria-label="Permalink to &quot;项目背景&quot;">​</a></h2><p>聊天机器人是一种能够与人类进行自然语言交互的智能系统， 它们在各个领域都有着广泛的应用和前景。随着大型语言模型（LLM）的出现和发展， 聊天机器人的性能和能力也得到了显著的提升。 LLM是一种利用大量文本数据来学习语言知识和规律的深度神经网络模型，它们可以生成流畅、连贯、有意义的文本， 甚至可以完成一些特定的任务或目标。目前，LLM已经成为了聊天机器人领域的主流技术之一，吸引了众多的研究者和开发者。</p><h2 id="项目介绍" tabindex="-1">项目介绍 <a class="header-anchor" href="#项目介绍" aria-label="Permalink to &quot;项目介绍&quot;">​</a></h2><p>为了促进LLM在聊天机器人领域的发展和创新， <strong>LMSYS Org</strong> 创建了一个名为 <strong>Chatbot Arena</strong> 的平台。</p><h3 id="lmsys-org" tabindex="-1">LMSYS Org <a class="header-anchor" href="#lmsys-org" aria-label="Permalink to &quot;LMSYS Org&quot;">​</a></h3><p><strong>LMSYS Org</strong> 是一个由加州大学伯克利分校的学生和教师组成的开放研究组织， 它旨在通过共同开发的方式，使大型模型更加易于使用和可访问。</p><h3 id="chatbot-arena" tabindex="-1">Chatbot Arena <a class="header-anchor" href="#chatbot-arena" aria-label="Permalink to &quot;Chatbot Arena&quot;">​</a></h3><p><strong>Chatbot Arena</strong> 是一个大型语言模型（LLM）的基准平台， 它展示了不同的聊天机器人模型在与真实用户对话中的性能和评分。 不同的机器人以众包方式提供匿名，随机的战斗， 该平台计划每周更新一次排行榜，根据模型与用户对话的结果来计算模型的Elo评分。</p><h3 id="elo-评分" tabindex="-1">Elo 评分 <a class="header-anchor" href="#elo-评分" aria-label="Permalink to &quot;Elo 评分&quot;">​</a></h3><p>Elo评分是一种衡量模型相对水平和优劣的指标，是国际象棋和其他竞技游戏中广泛使用的评级系统。</p><h2 id="流行的开源大型语言模型的-elo-评级" tabindex="-1">流行的开源大型语言模型的 Elo 评级 <a class="header-anchor" href="#流行的开源大型语言模型的-elo-评级" aria-label="Permalink to &quot;流行的开源大型语言模型的 Elo 评级&quot;">​</a></h2><p>表格在 手机上显示不太直观，先看图片。</p><p><img src="'+a+'" alt="流行的开源大型语言模型的 Elo 评级 230503"></p><table tabindex="0"><thead><tr><th>Rank</th><th>Model</th><th>Elo</th></tr></thead><tbody><tr><td>1</td><td>🥇 vicuna-13b</td><td>1169</td></tr><tr><td>2</td><td>🥈 koala-13b</td><td>1082</td></tr><tr><td>3</td><td>🥉 oasst-pythia-12b</td><td>1065</td></tr><tr><td>4</td><td>alpaca-13b</td><td>1008</td></tr><tr><td>5</td><td>chatglm-6b</td><td>985</td></tr><tr><td>6</td><td>fastchat-t5-3b</td><td>951</td></tr><tr><td>7</td><td>dolly-v2-12b</td><td>944</td></tr><tr><td>8</td><td>llama-13b</td><td>932</td></tr><tr><td>9</td><td>stablelm-tuned-alpha-7b</td><td>858</td></tr></tbody></table><p>表 1 显示了九种流行型号的 Elo 评级， 这些模型基于本笔记本中共享的 4.7K 投票数据和计算。 您还可以尝试投票演示并查看最新的排行榜。</p><p><img src="'+d+'" alt="并排聊天和投票界面"></p><h2 id="项目介绍-1" tabindex="-1">项目介绍 <a class="header-anchor" href="#项目介绍-1" aria-label="Permalink to &quot;项目介绍&quot;">​</a></h2><p>随着 ChatGPT 的巨大成功，开源大型语言模型激增，这些模型经过微调以遵循说明。 这些模型能够为响应用户的问题/提示提供有价值的帮助。 值得注意的例子包括基于 <strong>LLaMA</strong> 的 <strong>Alpaca</strong> 和 <strong>Vicuna</strong> ，以及基于 <strong>Pythia</strong> 的 <strong>OpenAssistant</strong> 和 <strong>Dolly</strong> 。</p><p>尽管每周都会不断发布新模型，但社区在有效对这些模型进行基准测试方面面临着挑战。 对LLM助手进行基准测试极具挑战性，因为问题可能是开放式的，并且很难编写程序来自动评估响应质量。 在这种情况下，我们通常必须诉诸基于成对比较的人工评估。</p><h3 id="对比系统属性" tabindex="-1">对比系统属性 <a class="header-anchor" href="#对比系统属性" aria-label="Permalink to &quot;对比系统属性&quot;">​</a></h3><p>基于成对比较的良好基准系统有一些所需的属性：</p><ul><li>可扩展性：当无法为所有可能的模型对收集足够的数据时，系统应扩展到大量模型。</li><li>增长性：该系统应该能够使用相对较少的试验来评估新模型。</li><li>唯一顺序：系统应为所有模型提供唯一的顺序。给定任何两个模型，我们应该能够分辨出哪个排名更高或它们是否并列。</li></ul><h3 id="chatbot-arena-1" tabindex="-1">Chatbot Arena <a class="header-anchor" href="#chatbot-arena-1" aria-label="Permalink to &quot;Chatbot Arena&quot;">​</a></h3><p>现有的LLM基准系统很少满足所有这些属性。经典的LLM基准框架， 如HELM和lm-evaluation-harness，为学术研究中常用的任务提供多指标测量。 但是，它们不是基于成对比较，在评估开放式问题方面无效。OpenAI还启动了evals项目来收集更好的问题， 但该项目并未为所有参与模型提供排名机制。当我们推出骆马模型时，我们使用了基于 GPT-4 的评估管道， 但它没有为可扩展和增量评级提供解决方案。</p><p>在这篇博文中，我们介绍了 <strong>Chatbot Arena</strong>，这是一个LLM基准平台，以众包方式进行匿名随机战斗。聊天机器人竞技场采用Elo评级系统，这是国际象棋和其他竞技游戏中广泛使用的评级系统。Elo评级系统有望提供上述所需的属性。我们注意到Anthropic LLM论文也采用了Elo评级系统。</p><p>为了收集数据，我们在一周前推出了几个流行的开源LLM的竞技场。在竞技场中，用户可以并排与两个匿名模型聊天，并投票选出哪个更好。这种众包数据收集方式代表了LLM在野外的一些用例。几种评估方法之间的比较如表2所示。</p><h2 id="评估方法和说明" tabindex="-1">评估方法和说明 <a class="header-anchor" href="#评估方法和说明" aria-label="Permalink to &quot;评估方法和说明&quot;">​</a></h2><h3 id="不同评估方法之间的比较" tabindex="-1">不同评估方法之间的比较 <a class="header-anchor" href="#不同评估方法之间的比较" aria-label="Permalink to &quot;不同评估方法之间的比较&quot;">​</a></h3><table tabindex="0"><thead><tr><th><br></th><th>HELM / lm-evaluation-harness</th><th>OpenAI/eval</th><th>Alpaca 评估</th><th>Vicuna 评估</th><th>Chatbot Arena</th></tr></thead><tbody><tr><td>问题原</td><td>学术数据集</td><td>混合</td><td>自学评估集</td><td>生成 GPT-4</td><td>用户提示</td></tr><tr><td>评估器</td><td>程序</td><td>程序/模型</td><td>人</td><td>GPT-4</td><td>用户</td></tr><tr><td>指标</td><td>基本指标</td><td>基本指标</td><td>胜率</td><td>胜率</td><td>Elo 评级</td></tr></tbody></table><h3 id="数据采集" tabindex="-1">数据采集 <a class="header-anchor" href="#数据采集" aria-label="Permalink to &quot;数据采集&quot;">​</a></h3><p>我们在 <a href="https://arena.lmsys.org/" target="_blank" rel="noreferrer">https://arena.lmsys.org</a> 使用我们的多模型服务系统<a href="https://github.com/lm-sys/FastChat" target="_blank" rel="noreferrer">FastChat</a>主持了竞技场。 当用户进入竞技场时，他们可以并排与两个匿名模型聊天，如图 1 所示。 在得到两个模型的响应后， 用户可以继续聊天或投票给他们认为更好的模型。提交投票后，模型名称将被揭示。 用户可以继续聊天或使用两个随机选择的新匿名模型重新开始新的战斗。 该平台记录所有用户交互。在我们的分析中，我们仅在模型名称隐藏时使用投票。</p><p>该竞技场大约在一周前启动，从那时起我们已经收集了4.7k有效的匿名投票。 我们在本<a href="https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5?usp=sharing" target="_blank" rel="noreferrer">Notebook</a>中分享一些探索性分析，并在此处提供简短的摘要。</p><p><img src="https://lmsys.org/images/blog/arena/battle_counts.png" alt="每种模型组合的战斗计数"></p><p>上图显示了每种模型组合的战斗计数。当我们最初启动锦标赛时，我们根据我们的基准预先获得了有关可能排名的信息， 并选择根据该排名配对模型。根据这个排名，我们优先考虑我们认为会是强配对。但是，我们后来切换到统一抽样，以获得更好的排名整体覆盖率。在比赛即将结束时，我们还推出了一款新车型。所有这些都会导致模型频率不均匀。<code>fastchat-t5-3b</code></p><p><img src="https://lmsys.org/images/blog/arena/lang_counts.png" alt="前 15 种语言的战斗计数"></p><p>上图绘制了语言分布图，并显示大多数用户提示都是英语。</p><h3 id="埃洛评级系统" tabindex="-1">埃洛评级系统 <a class="header-anchor" href="#埃洛评级系统" aria-label="Permalink to &quot;埃洛评级系统&quot;">​</a></h3><p><a href="https://en.wikipedia.org/wiki/Elo_rating_system" target="_blank" rel="noreferrer">Elo评级系统</a>是一种计算球员相对技能水平的方法， 已在竞技游戏和体育中广泛采用。两个玩家之间的评分差异可以作为比赛结果的预测指标。 Elo 评级系统非常适合我们的情况，因为我们有多个模型，并且在它们之间进行成对战斗。</p><h2 id="成对赢率" tabindex="-1">成对赢率 <a class="header-anchor" href="#成对赢率" aria-label="Permalink to &quot;成对赢率&quot;">​</a></h2><p>作为校准的基础，我们还在这里展示了锦标赛中每个模型的成对胜率（图 4）以及使用 Elo 评级估计的预测成对胜率（图 5）。 通过比较这些数字，我们发现elo评级可以相对很好地预测胜率。</p><p><img src="https://lmsys.org/images/blog/arena/win_fraction.png" alt="模型 A 在所有非平局的 A 与 B 战斗中获胜的比例"></p><p><img src="https://lmsys.org/images/blog/arena/predicted_win_fraction.png" alt="在 A 与 B 的战斗中使用 Elo 评级对模型 A 的预测胜率"></p><h2 id="项目" tabindex="-1">项目 <a class="header-anchor" href="#项目" aria-label="Permalink to &quot;项目&quot;">​</a></h2><p>项目地址：</p><ul><li><a href="https://lmsys.org/" target="_blank" rel="noreferrer">https://lmsys.org/</a></li></ul><hr><div style="text-align:center;color:#00000099;font-size:14px;">END</div>',50)]))}const u=r(s,[["render",n]]);export{m as __pageData,u as default};
