import{_ as i,a,af as t,o as n}from"./chunks/framework.C87LdZyP.js";const e="/assets/204953980246208.CkQibqpR.png",c=JSON.parse('{"title":"从 Graph 到 EDA，LlamaIndex 复合AI系统的实现方案","description":"","frontmatter":{"title":"从 Graph 到 EDA，LlamaIndex 复合AI系统的实现方案","date":"2024-08-03 10:00:00","tags":["llamaindex","ai-agent"],"category":["AI"]},"headers":[],"relativePath":"posts/2024/08/2024-08-03-llamaindex-workflow.md","filePath":"posts/2024/08/2024-08-03-llamaindex-workflow.md","lastUpdated":1722996316000}'),l={name:"posts/2024/08/2024-08-03-llamaindex-workflow.md"};function p(h,s,r,k,d,o){return n(),a("div",null,s[0]||(s[0]=[t(`<p>前些天 LlamaIndex 推出了 Workflow 的实现， 旨在解决构建复杂 Agent AI 中遇到的问题。</p><h2 id="llamaindex-工作流" tabindex="-1">LlamaIndex 工作流 <a class="header-anchor" href="#llamaindex-工作流" aria-label="Permalink to &quot;LlamaIndex 工作流&quot;">​</a></h2><p>LlamaIndex 工作流是一种在<strong>用户构建的日益复杂的 AI 应用程序中协调操作的机制</strong>。</p><h3 id="背景" tabindex="-1">背景 <a class="header-anchor" href="#背景" aria-label="Permalink to &quot;背景&quot;">​</a></h3><p>随着 LLM 的出现，人工智能应用程序由<strong>多个任务</strong>组成， 这些任务由<strong>不同的组件</strong>实现，这已成为事实上的标准。</p><p>市场上的开源框架致力于通过提供易于使用的基础组件抽象 （如数据加载器、LLM、向量数据库和重新排序器）以及外部服务， 让人工智能工程师的工作更加轻松。 与此同时，所有<strong>这些框架也在寻求最佳抽象来协调这些组件</strong>， 研究对人工智能开发人员来说最直观、最有效的方法， 以实现将复合人工智能系统结合在一起的逻辑。</p><h2 id="现有的解决方案" tabindex="-1">现有的解决方案 <a class="header-anchor" href="#现有的解决方案" aria-label="Permalink to &quot;现有的解决方案&quot;">​</a></h2><p>其中两种潜在的编排模式是<strong>链</strong>和<strong>管道</strong>，它们都是相同<strong>有向无环图 (DAG)</strong> 抽象的实现。</p><p>年初发布的<a href="https://www.llamaindex.ai/blog/introducing-query-pipelines-025dc2bb0537" target="_blank" rel="noreferrer">查询管道(Query Pipeline)</a> 中也尝试了这一点。</p><div class="admonition note"><p class="admonition-title">查询管道</p><p><strong>查询管道</strong>是一个声明性 API， 可让您针对不同用例（如 QA、结构化提取和代理自动化）编排数据的从简单到高级的查询工作流程。</p></div><h2 id="基于graph-dag-的局限性" tabindex="-1">基于Graph（DAG）的局限性 <a class="header-anchor" href="#基于graph-dag-的局限性" aria-label="Permalink to &quot;基于Graph（DAG）的局限性&quot;">​</a></h2><p>DAG 的一个基本方面是 DAG 中的『A』： 它们是非循环的，这意味着<strong>没有循环</strong>。</p><p>但在一个越来越具有代理性的世界里， 人工智能应用程序的逻辑中<strong>无法执行循环是完全不可接受的</strong>。 例如，如果一个组件提供了不好的结果，人工智能开发人员应该有办法告诉系统自我纠正并重试。</p><p>即使没有向 DAG 添加循环和重复（无方向），查询管道也存在一些明显的问题：</p><ul><li>出现问题时很难调试</li><li>它们掩盖了组件和模块的执行方式</li><li>我们的管道编排器变得越来越复杂，必须处理大量不同的边缘情况</li><li>对于复杂的管道来说，它们很难读取</li></ul><p>但是，一旦我们在查询管道中添加了循环，这些围绕图表的开发人员 <strong>用户体验</strong> 问题就会被放大。 我们在以下领域亲身体验了开发人员的痛苦：</p><ul><li>许多核心编排逻辑（如 <code>if-else </code>语句和<code>while</code>循环）都嵌入到图的边缘中。定义这些边缘变得繁琐而冗长。</li><li>处理可选值和默认值的极端情况变得困难。作为一个框架，我们很难确定参数是否会从上游节点传递过来。</li><li>对于构建代理的开发人员来说，定义带有循环的图表并不总是那么自然。代理封装了一个由 LLM 驱动的通用实体，该实体可以接收观察并生成响应。在这里，图表 UX 强制“代理”节点明确定义传入边和传出边，迫使用户定义与其他节点的详细通信模式。</li></ul><h2 id="从图到eda-走向事件驱动" tabindex="-1">从图到EDA：走向事件驱动 <a class="header-anchor" href="#从图到eda-走向事件驱动" aria-label="Permalink to &quot;从图到EDA：走向事件驱动&quot;">​</a></h2><div class="admonition note"><p class="admonition-title">EDA</p><p>EDA 是 『<strong>Event-Driven Architecture</strong>』 的缩写，即<strong>事件驱动架构</strong>。 这种架构模式强调系统对事件的响应，而不是由一个中心化的控制流程驱动。</p></div><p><strong>复合 AI 系统</strong>可通过 LlamaIndex工作流实现。 工作流通过一组称为步骤的 Python 函数来回调度事件。 每个步骤可视为系统的一个组件：</p><ul><li>一个用于处理查询，</li><li>一个用于与 LLM 对话，</li><li>一个用于从矢量数据库加载数据，等等。</li></ul><p>每个步骤都会接收一个或多个要处理的事件，并且可以选择性地发回将在需要时中继到其他组件的事件。</p><p>转向事件驱动架构会导致<strong>设计发生根本性转变</strong>。 在许多图形实现中，图形遍历算法负责确定接下来应运行哪个组件以及应传递哪些数据。 在事件驱动架构中，组件会订阅某些类型的事件，并最终负责根据收到的数据决定要做什么。</p><h2 id="工作流入门" tabindex="-1">工作流入门 <a class="header-anchor" href="#工作流入门" aria-label="Permalink to &quot;工作流入门&quot;">​</a></h2><p>我们来看一个例子。最小的 LlamaIndex 工作流程如下所示（为了方便查看，我们隐藏包导入的部分）：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;"> OpenAIGenerator</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#6CB6FF;">Workflow</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">):</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#DCBDFB;">    @step</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">    async</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#DCBDFB;"> generate</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">(self, ev: StartEvent) -&gt; StopEvent:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">        query </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> ev.get(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;query&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">        llm </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> OpenAI()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">        response </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> llm.acomplete(query)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> StopEvent(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">result</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">str</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">(response))</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">w </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> OpenAIGenerator(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">timeout</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">verbose</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">result </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> w.run(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">query</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;What&#39;s LlamaIndex?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">(result)</span></span></code></pre></div><p>使用装饰器<code>@step</code>标记<code>generate</code>函数为工作流的一个步骤， 并使用带有适当类型注释的方法签名声明它想要接收哪些事件以及它将发送回哪些事件。</p><p>为了运行工作流， 我们创建<code>OpenAIGenerator</code>类的实例并传递一些配置参数（例如所需的超时时间），</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">w </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> OpenAIGenerator(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">timeout</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">verbose</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span></code></pre></div><p>然后调用该示例的<code>run</code>方法。</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">result </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> w.run(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">query</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;What&#39;s LlamaIndex?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span></code></pre></div><p>我们在看 <code>generate</code> 方法的实现：</p><ul><li>传递给 <code>run</code> 的任何关键字参数，都将被打包到 <code>StartEvent</code> 中；</li><li>通过 <code>ev.get(&quot;query&quot;)</code> 获取参数；</li><li>返回一个特殊类型的事件<code>StopEvent</code>，向工作流发出信号以正常停止其执行；</li><li><code>StopEvent</code> 携带工作流结果返回给调用者，比如本例中的 LLM 响应。</li></ul><p>复杂的用法可以参考 LlamaIndex 的文档：</p><ul><li><a href="https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex" target="_blank" rel="noreferrer">Introducing workflows</a></li><li><a href="https://docs.llamaindex.ai/en/latest/module_guides/workflow/" target="_blank" rel="noreferrer">Workflow guides</a></li></ul><h2 id="调试工作流程" tabindex="-1">调试工作流程 <a class="header-anchor" href="#调试工作流程" aria-label="Permalink to &quot;调试工作流程&quot;">​</a></h2><p>工作流程的复杂性将随着应用程序逻辑的复杂性而增长， 有时仅通过查看 Python 代码可能很难理解事件在执行过程中的流动方式。</p><p>为了便于理解复杂的工作流程并支持工作流程执行的调试，LlamaIndex 提供了两个函数：</p><ul><li><code>draw_all_possible_flows</code> 生成一幅图，展示工作流程中的所有步骤以及事件可能如何流动</li><li><code>draw_most_recent_execution</code> 生成类似的图片，仅显示上次工作流执行期间实际发送的事件</li></ul><p><img src="`+e+'" alt=""></p><h2 id="工作流的功能" tabindex="-1">工作流的功能 <a class="header-anchor" href="#工作流的功能" aria-label="Permalink to &quot;工作流的功能&quot;">​</a></h2><p>工作流具有一组您通常会期望从更成熟的软件中获得的功能：</p><ul><li>完全异步，支持流媒体</li><li>默认进行检测，通过支持的集成提供一键可观察性</li><li>逐步执行，更易于调试</li><li>事件驱动依赖关系的验证和可视化</li><li>事件被实现为 pydantic 模型，以简化新功能的定制和进一步开发</li></ul><div class="admonition note"><p class="admonition-title">Pydantic模型</p><p>Pydantic 是 是 Python 中使用最广泛的数据验证框架。他可以定义数据类型， 方便和各种工具集成。比如上期我们使用 FastAPI 编写 Dify 自定义工具时就是用 Pydantic 来定义参数，从而生成 OpenAPI 文档。 更多信息可以查看 <a href="https://docs.pydantic.dev/latest/" target="_blank" rel="noreferrer">Pydantic文档</a>。</p></div>',44)]))}const A=i(l,[["render",p]]);export{c as __pageData,A as default};
