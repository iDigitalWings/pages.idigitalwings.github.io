import{_ as i,a as t,af as n,o as l}from"./chunks/framework.C87LdZyP.js";const a="/assets/693708909267625.3W95F10N.png",e="/assets/693962706994750.CTSRMhiK.png",h="/assets/696704247742166.CuKYxAVL.png",p="/assets/694136570565625.CvC9SPUR.png",d="/assets/694169073346458.BogDQziC.png",r="/assets/695600636960375.BFTyIUM4.png",u=JSON.parse('{"title":"langchain-ChatGLM: 使用清华大学大语言模型实现基于本地知识库的 ChatGLM 问答","description":"","frontmatter":{"title":"langchain-ChatGLM: 使用清华大学大语言模型实现基于本地知识库的 ChatGLM 问答","date":"2023-03-14T00:00:00.000Z","tags":["ai","ml"],"category":["AI"]},"headers":[],"relativePath":"posts/2023/03/2023-03-14-langchain-chatglm.md","filePath":"posts/2023/03/2023-03-14-langchain-chatglm.md","lastUpdated":1718173059000}'),o={name:"posts/2023/03/2023-03-14-langchain-chatglm.md"};function k(g,s,c,F,b,y){return l(),t("div",null,s[0]||(s[0]=[n('<blockquote><p>我们之前介绍过 <strong>ChatGLM 6B</strong>，今天介绍一个 langchain 结合 ChatGLM 6B 的本地知识库问答实现。 这是一种利用 <strong>langchain</strong> 思想实现的基于<strong>本地知识库</strong>的问答应用，目标期望建立一套对中文场景与<strong>开源</strong>模型支持友好、可<strong>离线运行</strong>的知识库问答解决方案。</p></blockquote><h2 id="预览" tabindex="-1">预览 <a class="header-anchor" href="#预览" aria-label="Permalink to &quot;预览&quot;">​</a></h2><p>部署之后，我把杨绛先生的《走到人生边上》这本书丢了进去，然后对这本书进行问答， 限于我投喂的知识内容，可以看到有些地方回答并不完美，但是出处还是找的挺好。</p><p><img src="'+a+'" alt=""></p><p>当然也支持使用 LLM 进行问答，或者联网搜索（最近新增加的功能，虽然表现有好有坏。）</p><p><img src="'+e+'" alt=""></p><p>模型配置界面：</p><p><img src="'+h+'" alt=""></p><h2 id="原理" tabindex="-1">原理 <a class="header-anchor" href="#原理" aria-label="Permalink to &quot;原理&quot;">​</a></h2><p>这里本地知识库问答的原理如下：</p><ol><li>-&gt;加载文件</li><li>-&gt; 读取文本</li><li>-&gt; 文本分割</li><li>-&gt; 文本向量化</li><li>-&gt; 问句向量化</li><li>-&gt; 在文本向量中匹配出与问句向量最相似的top k个</li><li>-&gt; 匹配出的文本作为上下文和问题一起添加到prompt中</li><li>-&gt; 提交给LLM生成回答。</li></ol><p>官网给出的流程图如下： <img src="'+p+'" alt="langchain ChatGLM 原理"></p><p>从文档处理角度来看，实现流程如下图： <img src="'+d+`" alt=""></p><h2 id="模型和硬件需求" tabindex="-1">模型和硬件需求 <a class="header-anchor" href="#模型和硬件需求" aria-label="Permalink to &quot;模型和硬件需求&quot;">​</a></h2><p>我们可使用如下模型：</p><ul><li>chatglm-6b-int4-qe</li><li>chatglm-6b-int4</li><li>chatglm-6b-int8</li><li>chatglm-6b</li><li>chatyuan</li><li>moss</li><li>vicuna-13b-hf</li><li>fastchat-chatglm-6b</li><li>fastchat-vicuna-13b-hf</li></ul><h3 id="chatglm-6b-模型" tabindex="-1">ChatGLM 6B 模型 <a class="header-anchor" href="#chatglm-6b-模型" aria-label="Permalink to &quot;ChatGLM 6B 模型&quot;">​</a></h3><p>我们之前介绍过 ChatGLM 6B 可以在消费级显卡上进行推理，而对显卡要求很低。</p><p>16位无量化需要 13GB 就可以进行推理：</p><table tabindex="0"><thead><tr><th><strong>量化等级</strong></th><th>FP16（无量化）</th></tr></thead><tbody><tr><td><strong>最低 GPU 显存</strong> <br>（推理）</td><td>13 GB</td></tr><tr><td><strong>最低 GPU 显存</strong> <br>（高效参数微调）</td><td>14 GB</td></tr></tbody></table><p>INT8 则需要 8GB 显存：</p><table tabindex="0"><thead><tr><th><strong>量化等级</strong></th><th>INT8</th></tr></thead><tbody><tr><td><strong>最低 GPU 显存</strong> <br>（推理）</td><td>8 GB</td></tr><tr><td><strong>最低 GPU 显存</strong> <br>（高效参数微调）</td><td>9 GB</td></tr></tbody></table><p>INT4 只需要 6GB 就可以进行推理：</p><table tabindex="0"><thead><tr><th><strong>量化等级</strong></th><th>INT4</th></tr></thead><tbody><tr><td><strong>最低 GPU 显存</strong> <br>（推理）</td><td>6 GB</td></tr><tr><td><strong>最低 GPU 显存</strong> <br>（高效参数微调）</td><td>7 GB</td></tr></tbody></table><h3 id="moss-模型" tabindex="-1">MOSS 模型 <a class="header-anchor" href="#moss-模型" aria-label="Permalink to &quot;MOSS 模型&quot;">​</a></h3><p>也可以使用复旦大学的 MOSS 模型，至少需要 20G 显存。</p><p>FP16 需要 68G 显存：</p><table tabindex="0"><thead><tr><th><strong>量化等级</strong></th><th>FP16（无量化）</th></tr></thead><tbody><tr><td><strong>最低 GPU 显存</strong> <br>（推理）</td><td>68 GB</td></tr></tbody></table><p>INT8的量化需要 20G 显存：</p><table tabindex="0"><thead><tr><th><strong>量化等级</strong></th><th>INT8</th></tr></thead><tbody><tr><td><strong>最低 GPU 显存</strong> <br>（推理）</td><td>20 GB</td></tr></tbody></table><h3 id="embedding-模型" tabindex="-1">Embedding 模型 <a class="header-anchor" href="#embedding-模型" aria-label="Permalink to &quot;Embedding 模型&quot;">​</a></h3><p>本项目中默认选用的 Embedding 模型 <code>GanymedeNil/text2vec-large-chinese</code> 约占用显存 3GB，也可修改为在 CPU 中运行。</p><h2 id="开发运行" tabindex="-1">开发运行 <a class="header-anchor" href="#开发运行" aria-label="Permalink to &quot;开发运行&quot;">​</a></h2><p>项目开发环境建议在 <code>Python 3.8.1 - 3.10</code>，<code>CUDA 11.7</code> 环境下完成进行。</p><h3 id="安装环境" tabindex="-1">安装环境 <a class="header-anchor" href="#安装环境" aria-label="Permalink to &quot;安装环境&quot;">​</a></h3><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># 拉取仓库</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">$</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> git</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> clone</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> https://github.com/imClumsyPanda/langchain-ChatGLM.git</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># 进入目录</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">$</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> cd</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> langchain-ChatGLM</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># 项目中 pdf 加载由先前的 detectron2 替换为使用 paddleocr，如果之前有安装过 detectron2 需要先完成卸载避免引发 tools 冲突</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">$</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> pip</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> uninstall</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> detectron2</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># 检查paddleocr依赖，linux环境下paddleocr依赖libX11，libXext</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">$</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> yum</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> libX11</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">$</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> yum</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> libXext</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># 安装依赖</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">$</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> pip</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -r</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> requirements.txt</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># 验证paddleocr是否成功，首次运行会下载约18M模型到~/.paddleocr</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">$</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> loader/image_loader.py</span></span></code></pre></div><h3 id="运行" tabindex="-1">运行 <a class="header-anchor" href="#运行" aria-label="Permalink to &quot;运行&quot;">​</a></h3><p>执行 cli_demo.py 脚本体验命令行交互：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">$</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> cli_demo.py</span></span></code></pre></div><p>或执行 webui.py 脚本体验 Web 交互</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">$</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> webui.py</span></span></code></pre></div><p>界面如下：</p><p><img src="`+a+`" alt=""></p><p>或执行 api.py 利用 fastapi 部署 API</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">$</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> api.py</span></span></code></pre></div><p>或成功部署 API 后，执行以下脚本体验基于 VUE 的前端页面</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">$</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> cd</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> views</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">$</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> pnpm</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> i</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">$</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> npm</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> dev</span></span></code></pre></div><p>界面如下：</p><p><img src="`+r+'" alt=""></p><hr><div style="text-align:center;color:#00000099;font-size:14px;">END</div>',51)]))}const D=i(o,[["render",k]]);export{u as __pageData,D as default};
