import{_ as a,a as e,o as t,aj as r}from"./chunks/framework.Ba_Ek9Jm.js";const s="/assets/108473165181166.CB9FXLA_.png",o="/assets/108561339500333.DDbBKzaa.png",n="/assets/108571747531958.C1l8i5U2.png",i="/assets/108625011080833.CqZQigfj.png",l="/assets/108848820936250.DpHE1TUU.gif",p="/assets/109049450043375.vfC9gafm.png",C=JSON.parse('{"title":"CVPR 2023 候选获奖论文都讲了什么(4/4)","description":"","frontmatter":{"title":"CVPR 2023 候选获奖论文都讲了什么(4/4)","date":"2023-04-14T00:00:00.000Z","tags":["ai","ml"],"category":["AI"]},"headers":[],"relativePath":"posts/2023/04/2023-04-14-cvpr-2023-Award-Candidates-4.md","filePath":"posts/2023/04/2023-04-14-cvpr-2023-Award-Candidates-4.md","lastUpdated":1718173059000}'),c={name:"posts/2023/04/2023-04-14-cvpr-2023-Award-Candidates-4.md"},h=r('<blockquote><p>《CVPR 2023 候选获奖论文都讲了什么》第四篇，介绍一下物体检测等三篇论文。</p></blockquote><h2 id="《what-can-human-sketches-do-for-object-detection-》" tabindex="-1">《What Can Human Sketches Do for Object Detection?》 <a class="header-anchor" href="#《what-can-human-sketches-do-for-object-detection-》" aria-label="Permalink to &quot;《What Can Human Sketches Do for Object Detection?》&quot;">​</a></h2><p>《手绘草图可以为物体检测做什么？》</p><p>这个没啥好解释的，直接看效果：</p><p><img src="'+s+'" alt=""></p><p>而且还支持类检测和细粒度物体检测、以及部分物体检测。</p><h3 id="种类检测" tabindex="-1">种类检测 <a class="header-anchor" href="#种类检测" aria-label="Permalink to &quot;种类检测&quot;">​</a></h3><p><img src="'+o+'" alt=""></p><h3 id="细粒度" tabindex="-1">细粒度 <a class="header-anchor" href="#细粒度" aria-label="Permalink to &quot;细粒度&quot;">​</a></h3><p>只检测同类、同动作的物体。</p><p><img src="'+n+'" alt=""></p><h3 id="部分物体检测" tabindex="-1">部分物体检测 <a class="header-anchor" href="#部分物体检测" aria-label="Permalink to &quot;部分物体检测&quot;">​</a></h3><p><img src="'+i+'" alt=""></p><h3 id="地址" tabindex="-1">地址 <a class="header-anchor" href="#地址" aria-label="Permalink to &quot;地址&quot;">​</a></h3><p>论文：</p><ul><li><a href="https://arxiv.org/abs/2303.15149" target="_blank" rel="noreferrer">https://arxiv.org/abs/2303.15149</a> 项目：</li><li><a href="http://www.pinakinathc.me/sketch-detect/" target="_blank" rel="noreferrer">http://www.pinakinathc.me/sketch-detect/</a></li></ul><h2 id="《data-driven-feature-tracking-for-event-cameras》" tabindex="-1">《Data-driven Feature Tracking for Event Cameras》 <a class="header-anchor" href="#《data-driven-feature-tracking-for-event-cameras》" aria-label="Permalink to &quot;《Data-driven Feature Tracking for Event Cameras》&quot;">​</a></h2><p>《事件相机的数据驱动特性追踪》</p><p>直接看演示：</p><p><img src="'+l+'" alt=""></p><h3 id="摘要" tabindex="-1">摘要 <a class="header-anchor" href="#摘要" aria-label="Permalink to &quot;摘要&quot;">​</a></h3><p>由于其高时间分辨率、增强的运动模糊弹性和非常稀疏的输出，事件相机已被证明是低延迟和低带宽特征跟踪的理想选择，即使在具有挑战性的场景中也是如此。现有的事件相机特征跟踪方法要么是手工制作的，要么是从第一原理派生的，但需要大量的参数调整，对噪声敏感，并且由于未建模的影响而不能推广到不同的场景。为了解决这些缺陷，我们引入了第一个用于事件相机的数据驱动特征跟踪器，它利用低延迟事件来跟踪在灰度帧中检测到的特征。我们通过一个新颖的框架注意力模块实现了稳健的性能，该模块在特征轨道之间共享信息。通过直接将零样本从合成数据转移到真实数据，我们的数据驱动跟踪器在相对特征年龄方面比现有方法高出 120%，同时还实现了最低延迟。通过采用新颖的自我监督策略使我们的跟踪器适应真实数据，这种性能差距进一步扩大到 130%。</p><p>地址：</p><ul><li><a href="https://github.com/uzh-rpg/deep_ev_tracker" target="_blank" rel="noreferrer">https://github.com/uzh-rpg/deep_ev_tracker</a></li></ul><h2 id="《integral-neural-networks》" tabindex="-1">《Integral Neural Networks》 <a class="header-anchor" href="#《integral-neural-networks》" aria-label="Permalink to &quot;《Integral Neural Networks》&quot;">​</a></h2><p>《积分神经网络》</p><p>有兴趣大家取看看 PPT 吧，数学不好，没办法简单的解释给大家听。</p><p><img src="'+p+'" alt=""></p><h3 id="地址-1" tabindex="-1">地址 <a class="header-anchor" href="#地址-1" aria-label="Permalink to &quot;地址&quot;">​</a></h3><p>PPT 地址：</p><ul><li><a href="https://inn.thestage.ai/" target="_blank" rel="noreferrer">https://inn.thestage.ai/</a> 论文下载：</li><li><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Solodskikh_Integral_Neural_Networks_CVPR_2023_paper.pdf" target="_blank" rel="noreferrer">https://openaccess.thecvf.com/content/CVPR2023/papers/Solodskikh_Integral_Neural_Networks_CVPR_2023_paper.pdf</a></li></ul>',31),d=[h];function _(u,m,f,k,g,b){return t(),e("div",null,d)}const q=a(c,[["render",_]]);export{C as __pageData,q as default};
