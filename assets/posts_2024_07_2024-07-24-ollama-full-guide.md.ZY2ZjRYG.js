import{_ as i,a,af as l,o as t}from"./chunks/framework.C87LdZyP.js";const e="/assets/521789112676833.eRs5H5sZ.png",n="/assets/524521338916125.omHXRdAI.png",h="/assets/1057617592708.DfMLbN1z.png",p="/assets/500702378196458.CMUuEdx8.png",k="/assets/500818540273375.Bt35xobb.png",d="/assets/500902358916333.D8Nkl0jc.png",o="/assets/511600950780000.JG3mmyCk.png",r="/assets/511477190914750.D2G7k3AN.png",c="/assets/512600467778500.Cmy6jm24.png",g="/assets/515744127272208.BZ72h1BB.png",F="/assets/515974828174083.DYi3hbmk.png",y="/assets/518430133081500.DAr2Xu5K.png",u="/assets/521197411635166.CxwfYZe1.png",x=JSON.parse('{"title":"Ollama 本地运行大模型(LLM)完全指南","description":"","frontmatter":{"title":"Ollama 本地运行大模型(LLM)完全指南","date":"2024-07-24 10:00:00","tags":["llm-local-api"],"category":["AI"]},"headers":[],"relativePath":"posts/2024/07/2024-07-24-ollama-full-guide.md","filePath":"posts/2024/07/2024-07-24-ollama-full-guide.md","lastUpdated":1721917751000}'),m={name:"posts/2024/07/2024-07-24-ollama-full-guide.md"};function D(C,s,b,v,A,q){return t(),a("div",null,s[0]||(s[0]=[l('<div class="admonition abstract"><p class="admonition-title">abstract</p><p>文章介绍了 Ollama 本地运行大模型（LLM）的方方面面， 包括安装运行、对话、自定义模型、系统提示配置、调试、开发、存储、如何作为服务、OpenAI 的兼容等。</p></div><p>这一年来，我已经习惯了使用线上大模型 API 来工作，只要网络在，就可以很方便地使用， 同时还能享受比较好的性能。</p><p>不过前两周的时候和一个客户聊系统，他们虽然现在没有应用大模型相关的能力，也没有计划安排 GPU 算力， 不过他们还是执着地要在本地进行大模型的部署。我想这也是很多企业不可改变的现状。</p><p>对于这部分需求，社区自然是已经有了<strong>很好而且很多</strong>的解决方案，比如 <strong>Ollama</strong>，这个 Github 已经 <strong>80.3K</strong> 星标的项目。 本来这类工具做的易用性非常好（简单），一般是拿来看官网文档直接用就好，不过我在使用的时候发现，他的官网是没有专门的文档页面， 只有连接到 Github 的 Markdown，搞得我连运行起来之后的默认端口都要问一下 AI。</p><p><img src="'+e+'" alt=""></p><p>所以我就想还是写篇文章，就把我看到的<strong>有用</strong>的信息都整理了一下，相信对大家也有点用，同时好让大家对 Ollama 有一个比较全面的了解。</p><p>文章包含以下内容：</p><ul><li>软件安装以及使用容器运行</li><li>模型下载、运行、对话</li><li>导入自定义模型</li><li>定制系统提示</li><li>CLI 命令全解</li><li>REST API 介绍</li><li>Python API 介绍</li><li>日志和 Debug</li><li>Ollama 作为一个服务使用</li><li>模型的存储</li><li>OpenAI 兼容性</li><li>并发等常见问题</li></ul><h2 id="ollama-介绍" tabindex="-1">Ollama 介绍 <a class="header-anchor" href="#ollama-介绍" aria-label="Permalink to &quot;Ollama 介绍&quot;">​</a></h2><p>Ollama是一个专为在本地机器上便捷部署和运行大型语言模型（LLM）而设计的开源工具。 它让用户无需深入了解复杂的底层技术，就能轻松地加载、运行和交互各种LLM模型。</p><div class="admonition note"><p class="admonition-title">本地和服务器</p><p>Ollama 最初是被设计为本地（主要是开发）运行LLM的工具，当然现在也可以在服务器（面向用户并发提供服务）上使用，并且兼容 OpenAI 接口，可以作为 OpenAI 的私有化部署方案。</p></div><p>Ollama 的特点：</p><ul><li>本地部署： 不依赖云端服务，用户可以在自己的设备上运行模型，保护数据隐私。</li><li>多操作系统支持：无论是 Mac、Linux 还是 Window，都能很方便安装使用。</li><li>多模型支持： Ollama 支持多种流行的LLM模型，如Llama、Falcon等，用户可以根据自己的需求选择不同的模型，一键运行。</li><li>易于使用： 提供了直观的命令行界面，操作简单，上手容易。</li><li>可扩展性： 支持自定义配置，用户可以根据自己的硬件环境和模型需求进行优化。</li><li>开源： 代码完全开放，用户可以自由查看、修改和分发，虽然没有很多人会去修改。</li></ul><h2 id="安装" tabindex="-1">安装 <a class="header-anchor" href="#安装" aria-label="Permalink to &quot;安装&quot;">​</a></h2><h3 id="macos" tabindex="-1">MacOS <a class="header-anchor" href="#macos" aria-label="Permalink to &quot;MacOS&quot;">​</a></h3><p>苹果电脑安装很简单，下载 Zip 解压，运行即可：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span>https://ollama.com/download/Ollama-darwin.zip</span></span></code></pre></div><p>安装运行 <code>Ollama.app</code> 之后，系统任务栏上会有一个应用程序图标，点击可以关闭 Ollama 服务。</p><p><img src="'+n+`" alt="MacOS 任务栏"></p><h3 id="windows" tabindex="-1">Windows <a class="header-anchor" href="#windows" aria-label="Permalink to &quot;Windows&quot;">​</a></h3><p>Windows 现在还处于预览版，官方也提供了安装包，安装过程不再赘述。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span>https://ollama.com/download/OllamaSetup.exe</span></span></code></pre></div><h3 id="linux" tabindex="-1">Linux <a class="header-anchor" href="#linux" aria-label="Permalink to &quot;Linux&quot;">​</a></h3><p>Linux 直接 Shell 脚本执行：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">curl</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -fsSL</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> https://ollama.com/install.sh</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> |</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;"> sh</span></span></code></pre></div><h3 id="docker" tabindex="-1">Docker <a class="header-anchor" href="#docker" aria-label="Permalink to &quot;Docker&quot;">​</a></h3><p>除了直接安装，我们还可以通过 Docker 运行，官方提供了镜像 <a href="https://hub.docker.com/r/ollama/ollama" target="_blank" rel="noreferrer">ollama/ollama</a>， 可以直接运行。</p><p>由于 Docker 有一层封装，所以使用 CPU 和 GPU 需要不同的配置。</p><h4 id="cpu-模式" tabindex="-1">CPU 模式 <a class="header-anchor" href="#cpu-模式" aria-label="Permalink to &quot;CPU 模式&quot;">​</a></h4><p>先说 CPU 模式运行，这个不需要什么配置和驱动，直接就可以运行，这也是 Ollama 的优势。</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># CPU 模式</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">docker</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> run</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -d</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -v</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ollama:/root/.ollama</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -p</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> 11434:11434</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --name</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ollama/ollama</span></span></code></pre></div><h4 id="nvidia-gpu" tabindex="-1">NVIDIA GPU <a class="header-anchor" href="#nvidia-gpu" aria-label="Permalink to &quot;NVIDIA GPU&quot;">​</a></h4><p>前面的命令运行时直接使用 CPU 来进行推理的，适合普通电脑没有 GPU 资源的场景。 如果想要在容器（Docker）内使用 GPU 进行推理，配置稍微麻烦一下。</p><p>在 Docker 中使用 Nvidia GPU 的过程大家可以参考 <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation" target="_blank" rel="noreferrer">NVIDIA 容器工具包文件安装文档</a>。</p><p>首先是为 Linux 发行版安装 NVIDIA GPU 驱动程序，安装过程也不是很复杂，以 APT（Ubuntu、Debian 等系统适用）为例。</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">curl</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -fsSL</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> https://nvidia.github.io/libnvidia-container/gpgkey</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> |</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;"> sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> gpg</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --dearmor</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -o</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">  &amp;&amp; </span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">curl</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -s</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -L</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> |</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">    sed</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> |</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">    sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> tee</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> /etc/apt/sources.list.d/nvidia-container-toolkit.list</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">sed</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -i</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -e</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;/experimental/ s/^#//g&#39;</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> /etc/apt/sources.list.d/nvidia-container-toolkit.list</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> apt-get</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> update</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> apt-get</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -y</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> nvidia-container-toolkit</span></span></code></pre></div><p>然后配置容器运行时，并重新启动容器服务进程（下面以 Docker 为例，Containerd/Kubernetes 可以参考文末文档）：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> nvidia-ctk</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> runtime</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> configure</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --runtime=docker</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> systemctl</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> restart</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> docker</span></span></code></pre></div><p>然后增加启动参数 <code>--gpus=all</code> 使用 GPU 模式启动容器：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># GPU 模式</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">docker</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> run</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -d</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --gpus=all</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -v</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ollama:/root/.ollama</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -p</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> 11434:11434</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --name</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ollama/ollama</span></span></code></pre></div><h4 id="amd-gpu" tabindex="-1">AMD GPU <a class="header-anchor" href="#amd-gpu" aria-label="Permalink to &quot;AMD GPU&quot;">​</a></h4><p>如果使用 AMD GPUs 来运行 Ollama， 使用 <code>rocm</code> 版本的镜像和以下命令运行：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">docker</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> run</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -d</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --device</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> /dev/kfd</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --device</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> /dev/dri</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -v</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ollama:/root/.ollama</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -p</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> 11434:11434</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --name</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ollama/ollama:rocm</span></span></code></pre></div><h2 id="运行模型" tabindex="-1">运行模型 <a class="header-anchor" href="#运行模型" aria-label="Permalink to &quot;运行模型&quot;">​</a></h2><h3 id="本地运行" tabindex="-1">本地运行 <a class="header-anchor" href="#本地运行" aria-label="Permalink to &quot;本地运行&quot;">​</a></h3><p>安装成功之后，运行模型只需要简单的一句命令：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> llama3.1</span></span></code></pre></div><p>如果是第一次运行， Ollama 会先从网络上下载模型（5个G左右），比如我下载速度是 2-5M/s，半个多小时之后就可以下载完成了。 等待过程大家可以泡个茶休息下。</p><p><img src="`+h+'" alt="第一次运行模型需要下载"></p><h3 id="容器运行" tabindex="-1">容器运行 <a class="header-anchor" href="#容器运行" aria-label="Permalink to &quot;容器运行&quot;">​</a></h3><p>在容器里面运行模型和在本机运行一样，使用 <code>ollama run &lt;model&gt;</code> 命令：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">docker</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> exec</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -it</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> llama3</span></span></code></pre></div><h2 id="支持的模型" tabindex="-1">支持的模型 <a class="header-anchor" href="#支持的模型" aria-label="Permalink to &quot;支持的模型&quot;">​</a></h2><p>使用 <code>ollama run &lt;模型名&gt;</code> 来运行模型，第一次运行会先下载模型哦。</p><p>比如下载 「Llama 3.1」， 那么执行下面命令即可：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> llama:3.1</span></span></code></pre></div><p>下面列出常用的模型，以及其参数数量、文件大小、在 Ollama 中的模型名，比如 Meta 公司的 Llama 模型， 谷歌公司的 Gemma 模型，以及国内智谱的 GLM 模型、阿里云的千问模型。。</p><table tabindex="0"><thead><tr><th>模型</th><th>参数</th><th>大小</th><th><code>模型名</code></th></tr></thead><tbody><tr><td>Llama3.1</td><td>8B</td><td>4.7G</td><td><code>llama3.1</code></td></tr><tr><td>Llama3.1</td><td>405B</td><td>231G</td><td><code>llama3.1:405b</code></td></tr><tr><td>GLM4</td><td>9B</td><td>5.5G</td><td><code>glm4</code></td></tr><tr><td>Qwen2</td><td>7B</td><td>4.4G</td><td><code>qwen2</code></td></tr><tr><td>Qwen2</td><td>72B</td><td>41G</td><td><code>qwen2:72b</code></td></tr><tr><td>Llama3</td><td>8B</td><td>4.7G</td><td><code>llama3</code></td></tr><tr><td>Llama3</td><td>70B</td><td>40G</td><td><code>llama3:70b</code></td></tr><tr><td>Phi3</td><td>3.8B</td><td>2.3G</td><td><code>phi3</code></td></tr><tr><td>Phi3</td><td>14B</td><td>7.9G</td><td><code>phi3:medium</code></td></tr><tr><td>Gemma2</td><td>9B</td><td>5.5G</td><td><code>gemma2</code></td></tr><tr><td>Gemma2</td><td>27B</td><td>16G</td><td><code>gemma2:27b</code></td></tr><tr><td>Mistral</td><td>7B</td><td>4.1G</td><td><code>mistral</code></td></tr><tr><td>Starling</td><td>7B</td><td>4.1G</td><td><code>starling-lm</code></td></tr><tr><td>CodeLlama</td><td>7B</td><td>3.8G</td><td><code>codellama</code></td></tr><tr><td>LLaVA</td><td>7B</td><td>4.5G</td><td><code>llava</code></td></tr><tr><td>Solar</td><td>10.7B</td><td>6.1G</td><td><code>solar</code></td></tr></tbody></table><p>完整的模型大家可以去 <a href="https://ollama.com/library" target="_blank" rel="noreferrer">Ollama Library</a> 去搜索和查看。</p><p><img src="'+p+'" alt="Ollama Library"></p><p>点击模型可以看到模型的介绍、参数列表、下载命令，等详细信息。</p><p><img src="'+k+'" alt="Ollama 模型介绍"></p><p>部分模型的 Readme 里面还可以看到模型的评估报告：</p><p><img src="'+d+'" alt="Ollama Llama 3.1 评估"></p><div class="admonition note"><p class="admonition-title">关于不同型号模型需要的内存</p><p>运行 <code>7B</code> 型号的模型，需要至少拥有 <strong>8 GB 的 RAM</strong> ，运行 <code>13B</code> 型号的模型需要至少 <strong>16 GB 的 RAM</strong>， ，运行 <code>33B</code> 型号的模型需要至少 <strong>32 GB 的 RAM</strong>。</p></div><h2 id="自定义模型" tabindex="-1">自定义模型 <a class="header-anchor" href="#自定义模型" aria-label="Permalink to &quot;自定义模型&quot;">​</a></h2><p>除了内置支持的模型，你也可以使用 Ollama 来运行自定义模型。 支持下面三种方式导入模型：</p><ul><li>GGUF</li><li>PyTorch</li><li>Safetensors</li></ul><h3 id="从-gguf-导入" tabindex="-1">从 GGUF 导入 <a class="header-anchor" href="#从-gguf-导入" aria-label="Permalink to &quot;从 GGUF 导入&quot;">​</a></h3><p>GGUF 是一种文件格式，用于存储使用 GGML 进行推理的模型以及基于 GGML 的执行器。</p><p>GGUF 是一种二进制格式，旨在快速加载和保存模型，并且易于读取。 一般情况下，模型是使用 PyTorch 或其他框架开发的，然后转换为 GGUF 以在 GGML 中使用。</p><p><img src="'+o+`" alt="GGUF格式架构"></p><p>GGUF 的更多介绍和特点我们不过多介绍，您可以访问 <a href="https://github.com/ggerganov/ggml" target="_blank" rel="noreferrer">GGUF Github 站点</a>来获取相关信息。</p><p>使用 Ollama 导入 GGUF 模型非常简单，只需要如下三步：</p><ol><li>创建一个名为 的文件Modelfile，其中包含FROM要导入的模型的本地文件路径的指令。</li></ol><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">FROM</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ./vicuna-33b.Q4_0.gguf</span></span></code></pre></div><ol start="2"><li>在 Ollama 中创建模型，这一步我们可以指定模型的名称，比如 <code>vicuna-33b-q4</code></li></ol><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> create</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> vicuna-33b-q4</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -f</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> Modelfile</span></span></code></pre></div><ol start="3"><li>经过上面两个步骤，我们就可以像运行内置模型一样运行刚刚导入的自定义模型了。</li></ol><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> vicuna-33b-q4</span></span></code></pre></div><h3 id="从-pytorch-或-safetensors-导入" tabindex="-1">从 PyTorch 或 Safetensors 导入 <a class="header-anchor" href="#从-pytorch-或-safetensors-导入" aria-label="Permalink to &quot;从 PyTorch 或 Safetensors 导入&quot;">​</a></h3><p>如果导入的模型是以下架构之一，则可以通过 Modelfile 直接导入 Ollama：</p><ul><li>LlamaForCausalLM</li><li>MistralForCausalLM</li><li>GemmaForCausalLM</li></ul><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">FROM</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> /path/to/safetensors/directory</span></span></code></pre></div><p>如果不能支持的话，需要先使用 <a href="https://github.com/ggerganov/llama.cpp/blob/master/README.md#prepare-and-quantize" target="_blank" rel="noreferrer">llama.cpp 转换成 GGUF 格式</a>：</p><p>常见的格式比如 HuggingFace、GGML、Lora 都支持，并提供了转换文件。</p><ul><li><code>convert_hf_to_gguf.py</code></li><li><code>convert_hf_to_gguf_update.py</code></li><li><code>convert_llama_ggml_to_gguf.py</code></li><li><code>convert_lora_to_gguf.py</code></li></ul><p>使用也很简单，比如转换 HuggingFace 模型。</p><p>首先克隆<code>llama.cpp</code>仓库，</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> clone</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> https://github.com/ggerganov/llama.cpp.git</span></span></code></pre></div><p>安装必要的 Python 依赖：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -r</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> llama.cpp/requirements.txt</span></span></code></pre></div><p>使用也很简单，传入 HuggingFace 的模型和输出位置等参数即可：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> llama.cpp/convert_hf_to_gguf.py</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> vicuna-hf</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">  --outfile</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> vicuna-13b-v1.5.gguf</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">  --outtype</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> q8_0</span></span></code></pre></div><h2 id="定制系统提示" tabindex="-1">定制系统提示 <a class="header-anchor" href="#定制系统提示" aria-label="Permalink to &quot;定制系统提示&quot;">​</a></h2><p>Ollama 支持运行系统模型时候，为每个模型定制系统提示（System Prompt）。</p><p>以智谱的 GLM 为例，来看看如何定制系统提示语。</p><p>首先确保我们下载了该模型：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> pull</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> glm4</span></span></code></pre></div><p>创建一个模型文件 <code>Modelfile</code>：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">FROM</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> glm4</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># set the temperature to 1 [higher is more creative, lower is more coherent]</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">PARAMETER</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> temperature</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 1</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># set the system message</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">SYSTEM</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &quot;&quot;&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">你现在是一个修仙世界的修炼导师，来指导人们修炼。每次回答都以：「渺小的人类」作为开始。</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;&quot;&quot;</span></span></code></pre></div><p>然后运行模型：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> create</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> glm4xiuxian</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -f</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ./Modelfile</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> glm4xiuxian</span></span></code></pre></div><p>然后问问他「今天我要做什么」，看看大模型怎么回答：</p><p><img src="`+r+`" alt="GLM4 系统提示词 - 修真版"></p><h2 id="ollama-相关命令" tabindex="-1">Ollama 相关命令 <a class="header-anchor" href="#ollama-相关命令" aria-label="Permalink to &quot;Ollama 相关命令&quot;">​</a></h2><p>前面已经用到了很多 Ollama 命令，我们下载系统的看一下 Ollama 的 CLI 命令。</p><h3 id="创建模型" tabindex="-1">创建模型 <a class="header-anchor" href="#创建模型" aria-label="Permalink to &quot;创建模型&quot;">​</a></h3><p><code>ollama create</code>用于从 Modelfile 创建模型。</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> create</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> mymodel</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -f</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ./Modelfile</span></span></code></pre></div><div class="admonition note"><p class="admonition-title">Modelfile</p><p><strong>Modelfile</strong> 是 模型的描述文件，类似用于构建 Docker 镜像的 <code>Dockerfile</code>。</p></div><h3 id="拉取和更新模型" tabindex="-1">拉取和更新模型 <a class="header-anchor" href="#拉取和更新模型" aria-label="Permalink to &quot;拉取和更新模型&quot;">​</a></h3><p><code>ollama pull</code> 用于拉取模型，如果模型已经存在，那么则更新本地模型。</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> pull</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> llama3</span></span></code></pre></div><h3 id="删除模型" tabindex="-1">删除模型 <a class="header-anchor" href="#删除模型" aria-label="Permalink to &quot;删除模型&quot;">​</a></h3><p>如果模型不再使用，可以使用 <code>ollama rm</code> 从本机删除掉，以节省磁盘存储空间。 下次使用的时候，仍然可以重新拉取。</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> rm</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> llama3</span></span></code></pre></div><h3 id="复制模型" tabindex="-1">复制模型 <a class="header-anchor" href="#复制模型" aria-label="Permalink to &quot;复制模型&quot;">​</a></h3><p><code>ollama cp</code> 命令可以复制一个模型，复制模型会占用双倍的磁盘空间。</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> cp</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> llama3</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> my-model</span></span></code></pre></div><h3 id="多行输入" tabindex="-1">多行输入 <a class="header-anchor" href="#多行输入" aria-label="Permalink to &quot;多行输入&quot;">​</a></h3><p>在 Ollama 的命令行里面，回车表示发送指令，如果需要多行输入，可以使用单引号 ( <code>&quot;&quot;&quot;</code> ) 来完成。</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">&gt;&gt;&gt; </span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">&quot;&quot;&quot;Hello,</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">... world!</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">... &quot;&quot;&quot;</span></span></code></pre></div><h3 id="多模态模型" tabindex="-1">多模态模型 <a class="header-anchor" href="#多模态模型" aria-label="Permalink to &quot;多模态模型&quot;">​</a></h3><p>使用多模态模型，只需要输入文件图片地址，即可表示上传了这个图片文件。</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">&gt;&gt;&gt; </span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">What</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">&#39;s in this image? /Users/jmorgan/Desktop/smile.png</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">The image features a yellow smiley face, which is likely the central focus of the picture.</span></span></code></pre></div><h3 id="run-参数提问" tabindex="-1">run 参数提问 <a class="header-anchor" href="#run-参数提问" aria-label="Permalink to &quot;run 参数提问&quot;">​</a></h3><p>我们可以把提示词直接通过参数传递给大模型，该命令不会进入交互式输入模式。 比如：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> glm4xiuxian</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> 今天我要做什么</span></span></code></pre></div><p><img src="`+c+`" alt=""></p><h3 id="显示模型信息" tabindex="-1">显示模型信息 <a class="header-anchor" href="#显示模型信息" aria-label="Permalink to &quot;显示模型信息&quot;">​</a></h3><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> show</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> glm4</span></span></code></pre></div><p>注意，只有下载过之后的模型才能通过 <code>show</code> 命令来显示模型相关信息。</p><h3 id="启动" tabindex="-1">启动 <a class="header-anchor" href="#启动" aria-label="Permalink to &quot;启动&quot;">​</a></h3><p>当您想启动 Ollama 时可以使用 <code>ollama serve</code>。这样就不需要使用 Ollama 的客户端程序了。 比如我们在 Linux 服务器上就可以这样使用。</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> serve</span></span></code></pre></div><p>启动之后，我们需要新开一个 Shell 窗口来运行模型：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">./ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> glm4</span></span></code></pre></div><div class="admonition note"><p class="admonition-title">默认端口</p><p>Ollama 服务的默认端口是 <code>11435</code>，无论是使用 App 启动，还是使用命令行启动，均是这个默认端口。</p></div><p>由于 命令行和 APP 都使用 <code>11435</code> 默认端口，我们再 GUI 工作的时候执行 <code>ollama server</code> 会提示端口已占用。 我们可以通过环境变量指定一个新的端口，来同时运行两个 ollama 实例：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">OLLAMA_HOST</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">127.0.0.1:11435</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;"> ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> serve</span></span></code></pre></div><p>Serve 命令提供了很多环境变量，可以让你更自由的运行 Ollama 程序：</p><ul><li><code>OLLAMA_DEBUG</code> 显示其他调试信息（例如 OLLAMA_DEBUG=1）</li><li><code>OLLAMA_HOST</code> ollama 服务器的 IP 地址（默认 127.0.0.1:11434）</li><li><code>OLLAMA_KEEP_ALIVE</code> 模型在内存中保持加载状态的持续时间（默认“5 分钟”）</li><li><code>OLLAMA_MAX_LOADED_MODELS</code> 每个 GPU 加载的最大模型数量</li><li><code>OLLAMA_MAX_QUEUE</code> 排队请求的最大数量</li><li><code>OLLAMA_MODELS</code> 模型目录的路径</li><li><code>OLLAMA_NUM_PARALLEL</code> 并行请求的最大数量</li><li><code>OLLAMA_NOPRUNE</code> 启动时不修剪模型 blob</li><li><code>OLLAMA_ORIGINS</code> 允许来源的逗号分隔列表</li><li><code>OLLAMA_TMPDIR</code> 临时文件的位置</li><li><code>OLLAMA_FLASH_ATTENTION</code> 启用闪存注意</li><li><code>OLLAMA_LLM_LIBRARY</code> 设置 LLM 库以绕过自动检测</li></ul><h2 id="rest-api" tabindex="-1">REST API <a class="header-anchor" href="#rest-api" aria-label="Permalink to &quot;REST API&quot;">​</a></h2><p>Ollama 启动之后，会自动运行一个执行和管理模型的 API。</p><h3 id="回答-api" tabindex="-1">回答 API <a class="header-anchor" href="#回答-api" aria-label="Permalink to &quot;回答 API&quot;">​</a></h3><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> http://localhost:11434/api/generate</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -d</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  &quot;model&quot;: &quot;llama3&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  &quot;prompt&quot;:&quot;Why is the sky blue?&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">}&#39;</span></span></code></pre></div><h3 id="聊天-api" tabindex="-1">聊天 API <a class="header-anchor" href="#聊天-api" aria-label="Permalink to &quot;聊天 API&quot;">​</a></h3><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> http://localhost:11434/api/chat</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -d</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  &quot;model&quot;: &quot;llama3&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  &quot;messages&quot;: [</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;why is the sky blue?&quot; }</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  ]</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">}&#39;</span></span></code></pre></div><p>除了上述最基本的用法，我们还可以指定响应格式、使用流式响应等。</p><p>比如使用流式 API：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> http://localhost:11434/api/generate</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -d</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  &quot;model&quot;: &quot;glm4&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  &quot;prompt&quot;: &quot;天空是什么颜色?&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">}&#39;</span></span></code></pre></div><p>或者非流式 API，直接返回结果：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> http://localhost:11434/api/generate</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -d</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  &quot;model&quot;: &quot;glm4&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  &quot;prompt&quot;: &quot;天空是什么颜色?&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  &quot;stream&quot;: false</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">}&#39;</span></span></code></pre></div><p>更多用法可以参考 <a href="https://github.com/ollama/ollama/blob/main/docs/api.md" target="_blank" rel="noreferrer">Ollama API 文档</a>。</p><h2 id="python-api" tabindex="-1">Python API <a class="header-anchor" href="#python-api" aria-label="Permalink to &quot;Python API&quot;">​</a></h2><p>Ollama 提供了 Python API，可以让 Python 程序快速与 Ollama 集成起来。</p><p>首先安装依赖库：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ollama</span></span></code></pre></div><p>聊天：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> ollama</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">response </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> ollama.chat(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;llama3&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">messages</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">[</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">  {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    &#39;role&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;user&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    &#39;content&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;Why is the sky blue?&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">  },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">])</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">(response[</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;message&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">][</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;content&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">])</span></span></code></pre></div><p>使用流式API：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> ollama</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">stream </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> ollama.chat(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">    model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;llama3&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">    messages</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">[{</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;role&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;user&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;content&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;Why is the sky blue?&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">}],</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">    stream</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> chunk </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> stream:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">  print</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">(chunk[</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;message&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">][</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;content&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">], </span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">end</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">flush</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span></code></pre></div><p>另外，CLI 提供的指令在 Python 里面都有 API 对应。</p><h3 id="聊天" tabindex="-1">聊天 <a class="header-anchor" href="#聊天" aria-label="Permalink to &quot;聊天&quot;">​</a></h3><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">ollama.create(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;example&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">modelfile</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">modelfile)</span></span></code></pre></div><h3 id="生成文本" tabindex="-1">生成文本 <a class="header-anchor" href="#生成文本" aria-label="Permalink to &quot;生成文本&quot;">​</a></h3><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">ollama.generate(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;llama3&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">prompt</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;Why is the sky blue?&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span></code></pre></div><h3 id="列出模型" tabindex="-1">列出模型 <a class="header-anchor" href="#列出模型" aria-label="Permalink to &quot;列出模型&quot;">​</a></h3><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">ollama.list()</span></span></code></pre></div><h3 id="显示模型信息-1" tabindex="-1">显示模型信息 <a class="header-anchor" href="#显示模型信息-1" aria-label="Permalink to &quot;显示模型信息&quot;">​</a></h3><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">ollama.show(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;llama3&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span></code></pre></div><h3 id="创建模型-1" tabindex="-1">创建模型 <a class="header-anchor" href="#创建模型-1" aria-label="Permalink to &quot;创建模型&quot;">​</a></h3><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">modelfile</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;&#39;&#39;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">FROM llama3</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">SYSTEM You are mario from super mario bros.</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;&#39;&#39;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">ollama.create(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;example&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">modelfile</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">modelfile)</span></span></code></pre></div><h3 id="拉取模型" tabindex="-1">拉取模型 <a class="header-anchor" href="#拉取模型" aria-label="Permalink to &quot;拉取模型&quot;">​</a></h3><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">ollama.pull(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;llama3&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span></code></pre></div><h3 id="拷贝模型" tabindex="-1">拷贝模型 <a class="header-anchor" href="#拷贝模型" aria-label="Permalink to &quot;拷贝模型&quot;">​</a></h3><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">ollama.copy(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;llama3&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;user/llama3&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span></code></pre></div><h3 id="删除模型-1" tabindex="-1">删除模型 <a class="header-anchor" href="#删除模型-1" aria-label="Permalink to &quot;删除模型&quot;">​</a></h3><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">ollama.delete(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;llama3&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span></code></pre></div><h3 id="自定义客户端" tabindex="-1">自定义客户端 <a class="header-anchor" href="#自定义客户端" aria-label="Permalink to &quot;自定义客户端&quot;">​</a></h3><p>创建客户端的时候可以指定 <code>host</code> 和 <code>timeout</code> 两个参数：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> ollama </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> Client</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">client </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> Client(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">host</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;http://localhost:11434&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">response </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> client.chat(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;llama3&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">messages</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">[</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">  {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    &#39;role&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;user&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    &#39;content&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;Why is the sky blue?&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">  },</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">])</span></span></code></pre></div><h3 id="错误处理" tabindex="-1">错误处理 <a class="header-anchor" href="#错误处理" aria-label="Permalink to &quot;错误处理&quot;">​</a></h3><p>一般使用 <code>ResponseError</code> 来捕获异常，处理错误：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">try</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">  ollama.chat(model)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">except</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> ollama.ResponseError </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> e:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">  print</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;Error:&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">, e.error)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">  if</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> e.status_code </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">==</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 404</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    ollama.pull(model)</span></span></code></pre></div><h2 id="日志" tabindex="-1">日志 <a class="header-anchor" href="#日志" aria-label="Permalink to &quot;日志&quot;">​</a></h2><p>Ollama 的日志文件存储在 <code>~/.ollama/logs/server.log</code>，我们可以通过查看 这个日志文件来获取 Ollama 的运行信息，以及排查错误。</p><p>在 Mac 上直接打开文件即可查看日志：</p><p><img src="`+g+'" alt=""></p><p>也可以通过命令行来查看：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">cat</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ~/.ollama/logs/server.log</span></span></code></pre></div><p><img src="'+F+`" alt=""></p><p>如果是容器运行的话，更简单，直接查看容器的输出即可：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">docker</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> logs</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> &lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">container-nam</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">e</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">&gt;</span></span></code></pre></div><h3 id="调试" tabindex="-1">调试 <a class="header-anchor" href="#调试" aria-label="Permalink to &quot;调试&quot;">​</a></h3><h2 id="llm-窗口大小" tabindex="-1">LLM 窗口大小 <a class="header-anchor" href="#llm-窗口大小" aria-label="Permalink to &quot;LLM 窗口大小&quot;">​</a></h2><p>默认情况下，Ollama 使用 2048 个标记的上下文窗口大小。</p><p>要在使用<code>ollama run</code> 的时候更改此设置，请使用<code>/set parameter</code>：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">/set</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> parameter</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> num_ctx</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 4096</span></span></code></pre></div><p>使用API时，指定num_ctx参数：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> http://localhost:11434/api/generate</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -d</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    &quot;model&quot;: &quot;llama3&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    &quot;prompt&quot;: &quot;Why is the sky blue?&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    &quot;options&quot;: {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">      &quot;num_ctx&quot;: 4096</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    }</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">}&#39;</span></span></code></pre></div><h2 id="gpu-使用" tabindex="-1">GPU 使用 <a class="header-anchor" href="#gpu-使用" aria-label="Permalink to &quot;GPU 使用&quot;">​</a></h2><p>通过 PS 命令查看 模型是否加载到了 GPU 上。</p><p>PS 命令会返回当前内存中已经加载的模型：</p><p><img src="`+y+`" alt=""></p><p><code>Processor</code> 列显示模型被加载到哪个内存中：</p><ul><li><code>100% GPU</code> 表示模型已完全加载到 GPU 中</li><li><code>100% CPU</code> 表示模型已完全加载到系统内存中</li><li><code>48%/52% CPU/GPU</code> 表示模型已部分加载到 GPU 和系统内存中</li></ul><h2 id="ollama-作为服务使用" tabindex="-1">Ollama 作为服务使用 <a class="header-anchor" href="#ollama-作为服务使用" aria-label="Permalink to &quot;Ollama 作为服务使用&quot;">​</a></h2><p>一直到前面为止， Ollama 运行起来都只能在本机使用，可以通过设置环境变量来让其他客户端访问。</p><h3 id="serve-命令" tabindex="-1">Serve 命令 <a class="header-anchor" href="#serve-命令" aria-label="Permalink to &quot;Serve 命令&quot;">​</a></h3><p>如果是使用 Serve 命令启动，那就比较简单，和我们前面改端口一样：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">OLLAMA_HOST</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">0.0.0.0:11435</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;"> ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> serve</span></span></code></pre></div><h3 id="macos-应用程序" tabindex="-1">MacOS 应用程序 <a class="header-anchor" href="#macos-应用程序" aria-label="Permalink to &quot;MacOS 应用程序&quot;">​</a></h3><p>如果 Ollama 作为 macOS 应用程序运行，则应使用以下命令设置环境变量launchctl：</p><p>使用 <code>launchctl setenv</code> 设置环境变量，然后重新运行程序。</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">launchctl</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> setenv</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> OLLAMA_HOST</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &quot;0.0.0.0&quot;</span></span></code></pre></div><h2 id="使用代理" tabindex="-1">使用代理 <a class="header-anchor" href="#使用代理" aria-label="Permalink to &quot;使用代理&quot;">​</a></h2><p>Ollama 支持使用 <code>HTTP_PROXY</code> 和 <code>HTTPS_PROXY</code> 来配置代理。</p><p>如果是 Docker 运行，那么使用 <code>-e HTTPS_PROXY=https://proxy.example.com</code> 来设置环境变量即可。</p><h2 id="模型存储" tabindex="-1">模型存储 <a class="header-anchor" href="#模型存储" aria-label="Permalink to &quot;模型存储&quot;">​</a></h2><p>Ollama 会下载模型到本地，不同操作系统的模型位置分别是：</p><ul><li>MacOS：<code>~/.ollama/models</code></li><li>Linux：<code>/usr/share/ollama/.ollama/models</code></li><li>Windows：<code>C:\\Users\\%username%\\.ollama\\models</code></li></ul><p>如果你想将模型存储到不同位置，可以使用 <code>OLLAMA_MODELS</code> 环境变量来指定不同的目录。</p><h2 id="并发" tabindex="-1">并发 <a class="header-anchor" href="#并发" aria-label="Permalink to &quot;并发&quot;">​</a></h2><p>以下服务器设置可用于调整 Ollama 在大多数平台上处理并发请求的方式：</p><ul><li><code>OLLAMA_MAX_LOADED_MODELS</code> 可同时加载的最大模型数量（前提是它们适合可用内存）。默认值为 3 * GPU 数量或 3（用于 CPU 推理）。</li><li><code>OLLAMA_NUM_PARALLEL</code> 每个模型同时处理的最大并行请求数。默认将根据可用内存自动选择 4 或 1。</li><li><code>OLLAMA_MAX_QUEUE</code> Ollama 在繁忙时排队的最大请求数，在拒绝其他请求之前。默认值为 512</li></ul><p>Ollama 处理并发请求的逻辑是：</p><ul><li>如果有足够内存，则可以同时加载多个模型</li><li>对于某个模型，如果有足够内存，则可以并发处理请求</li><li>如果内存不足以加载新模型：所有请求都排队，直至新模型加载</li><li>之前的模型空闲时，会写在一个或多个模型腾出空间给新模型</li><li>排队的请求按顺序处理</li></ul><h2 id="openai-兼容性" tabindex="-1">OpenAI 兼容性 <a class="header-anchor" href="#openai-兼容性" aria-label="Permalink to &quot;OpenAI 兼容性&quot;">​</a></h2><p>Ollama 与OpenAI API的部分内容提供了实验性的兼容性， 以帮助将现有应用程序连接到 Ollama。</p><h3 id="使用-openai-python-库" tabindex="-1">使用 OpenAI Python 库 <a class="header-anchor" href="#使用-openai-python-库" aria-label="Permalink to &quot;使用 OpenAI Python 库&quot;">​</a></h3><p>用法和 OpenAI 的客户端用法一样。</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">from</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> openai</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> import</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> OpenAI</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">client</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> =</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> OpenAI</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    base_url</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;http://localhost:11434/v1/&#39;,</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;">    # required but ignored</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    api_key</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;ollama&#39;,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">chat_completion</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> =</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> client.chat.completions.create</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    messages</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">[</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">        {</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">            &#39;role&#39;</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;user&#39;,</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">            &#39;content&#39;</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;Say this is a test&#39;,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">        }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    ],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;llama3&#39;,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span></code></pre></div><h3 id="openai-javascript" tabindex="-1">OpenAI JavaScript <a class="header-anchor" href="#openai-javascript" aria-label="Permalink to &quot;OpenAI JavaScript&quot;">​</a></h3><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">import</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> OpenAI</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> from</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;openai&#39;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">const</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> openai</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> =</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> new</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> OpenAI</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">({</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">  baseURL:</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;http://localhost:11434/v1/&#39;,</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">  //</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> required</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> but</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ignored</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">  apiKey:</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;ollama&#39;,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">})</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">const</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> chatCompletion</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> =</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> await</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> openai.chat.completions.create</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">({</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">  messages:</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> [{ </span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">role:</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;user&#39;,</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> content:</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;Say this is a test&#39;</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> }],</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">  model:</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;llama3&#39;,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">})</span></span></code></pre></div><h3 id="curl-工具" tabindex="-1">CURL 工具 <a class="header-anchor" href="#curl-工具" aria-label="Permalink to &quot;CURL 工具&quot;">​</a></h3><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> http://localhost:11434/v1/chat/completions</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">    -H</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &quot;Content-Type: application/json&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">    -d</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">        &quot;model&quot;: &quot;llama3&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">        &quot;messages&quot;: [</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">            {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">                &quot;role&quot;: &quot;system&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">                &quot;content&quot;: &quot;You are a helpful assistant.&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">            },</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">            {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">                &quot;role&quot;: &quot;user&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">                &quot;content&quot;: &quot;Hello!&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">            }</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">        ]</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    }&#39;</span></span></code></pre></div><h3 id="模型名称" tabindex="-1">模型名称 <a class="header-anchor" href="#模型名称" aria-label="Permalink to &quot;模型名称&quot;">​</a></h3><p>有些程序依赖 OpenAI 默认的模型名称（例如 <code>gpt-3.5-turbo</code>）， 无法修改或者指定模型，可以使用如下方法来解决。</p><p>复制一个你喜欢的模型为 <code>gpt-3.5-turbo</code>，</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> cp</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> llama3</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> gpt-3.5-turbo</span></span></code></pre></div><p>这样我们就可以李代桃僵来保证原有程序不变，依然传递 <code>gpt-3.5-turbo</code> 作为模型名称：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> http://localhost:11434/v1/chat/completions</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">    -H</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &quot;Content-Type: application/json&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">    -d</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">        &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">        &quot;messages&quot;: [</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">            {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">                &quot;role&quot;: &quot;user&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">                &quot;content&quot;: &quot;Hello!&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">            }</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">        ]</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">    }&#39;</span></span></code></pre></div><h2 id="社区和集成" tabindex="-1">社区和集成 <a class="header-anchor" href="#社区和集成" aria-label="Permalink to &quot;社区和集成&quot;">​</a></h2><p>社区有大量工具和类库都对 Ollama 做了集成，或者说基于 Ollama 进行构建， 我们可以在<a href="https://github.com/ollama/ollama/blob/main/README.md" target="_blank" rel="noreferrer">官网文档</a>上查看具体的项目列表：</p><p><img src="`+u+'" alt="Ollama 类库"></p>',247)]))}const E=i(m,[["render",D]]);export{x as __pageData,E as default};
