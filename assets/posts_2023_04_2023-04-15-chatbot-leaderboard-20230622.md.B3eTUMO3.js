import{_ as d,a,af as r,o as e}from"./chunks/framework.C87LdZyP.js";const o="/assets/156146266924791.G6D4yQEK.png",n="/assets/153636705498916.vghiUIxS.png",u=JSON.parse('{"title":"LMSYS 聊天机器人竞技排行榜 2023-06-22","description":"","frontmatter":{"title":"LMSYS 聊天机器人竞技排行榜 2023-06-22","date":"2023-04-15T00:00:00.000Z","tags":["ai","ml"],"category":["AI"]},"headers":[],"relativePath":"posts/2023/04/2023-04-15-chatbot-leaderboard-20230622.md","filePath":"posts/2023/04/2023-04-15-chatbot-leaderboard-20230622.md","lastUpdated":1718173059000}'),l={name:"posts/2023/04/2023-04-15-chatbot-leaderboard-20230622.md"};function i(h,t,s,c,p,B){return e(),a("div",null,t[0]||(t[0]=[r('<blockquote><p>为了促进LLM在聊天机器人领域的发展和创新， <strong>LMSYS Org</strong> 创建了一个名为 <strong>Chatbot Arena</strong> 的平台。 它展示了不同的聊天机器人模型在与真实用户对话中的性能和评分。 6月22号的更新中， 包含了更多的模型和三个指标。</p></blockquote><p>先看下排名：</p><table tabindex="0"><thead><tr><th>Model</th><th>MT-bench</th><th>Arena Elo</th><th>MMLU</th><th>License</th></tr></thead><tbody><tr><td>GPT-4</td><td>8.99</td><td>1227</td><td>86.4</td><td>私有</td></tr><tr><td>GPT-3.5-turbo</td><td>7.94</td><td>1130</td><td>70.0</td><td>私有</td></tr><tr><td>Claude-v1</td><td>7.90</td><td>1178</td><td>75.6</td><td>私有</td></tr><tr><td>Claude-instant-v1</td><td>7.85</td><td>1156</td><td>61.3</td><td>私有</td></tr><tr><td>Vicuna-33B</td><td>7.12</td><td>-</td><td>59.2</td><td>非商用</td></tr><tr><td>WizardLM-30B</td><td>7.01</td><td>-</td><td>58.7</td><td>非商用</td></tr><tr><td>Guanaco-33B</td><td>6.53</td><td>1065</td><td>57.6</td><td>非商用</td></tr><tr><td>Tulu-30B</td><td>6.43</td><td>-</td><td>58.1</td><td>非商用</td></tr><tr><td>Guanaco-65B</td><td>6.41</td><td>-</td><td>62.1</td><td>非商用</td></tr><tr><td>OpenAssistant-LLaMA-30B</td><td>6.41</td><td>-</td><td>56.0</td><td>非商用</td></tr><tr><td>PaLM-Chat-Bison-001</td><td>6.40</td><td>1038</td><td>-</td><td>私有</td></tr><tr><td>Vicuna-13B</td><td>6.39</td><td>1061</td><td>52.1</td><td>非商用</td></tr><tr><td>MPT-30B-chat</td><td>6.39</td><td>-</td><td>50.4</td><td>CC-BY-NC-SA-4.0</td></tr><tr><td>WizardLM-13B</td><td>6.35</td><td>1048</td><td>52.3</td><td>非商用</td></tr><tr><td>Vicuna-7B</td><td>6.00</td><td>1008</td><td>47.1</td><td>非商用</td></tr><tr><td>Baize-v2-13B</td><td>5.75</td><td>-</td><td>48.9</td><td>非商用</td></tr><tr><td>Nous-Hermes-13B</td><td>5.51</td><td>-</td><td>49.3</td><td>非商用</td></tr><tr><td>MPT-7B-Chat</td><td>5.42</td><td>956</td><td>32.0</td><td>CC-BY-NC-SA-4.0</td></tr><tr><td>GPT4All-13B-Snoozy</td><td>5.41</td><td>986</td><td>43.0</td><td>非商用</td></tr><tr><td>Koala-13B</td><td>5.35</td><td>992</td><td>44.7</td><td>非商用</td></tr><tr><td>MPT-30B-Instruct</td><td>5.22</td><td>-</td><td>47.8</td><td>CC-BY-SA 3.0</td></tr><tr><td>Falcon-40B-Instruct</td><td>5.17</td><td>-</td><td>54.7</td><td>Apache2.0</td></tr><tr><td>H2O-Oasst-OpenLLaMA-13B</td><td>4.63</td><td>-</td><td>42.8</td><td>Apache2.0</td></tr><tr><td>Alpaca-13B</td><td>4.53</td><td>930</td><td>48.1</td><td>非商用</td></tr><tr><td>ChatGLM-6B</td><td>4.50</td><td>905</td><td>36.1</td><td>非商用</td></tr><tr><td>OpenAssistant-Pythia-12B</td><td>4.32</td><td>924</td><td>27.0</td><td>Apache2.0</td></tr><tr><td>RWKV-4-Raven-14B</td><td>3.98</td><td>950</td><td>25.6</td><td>Apache2.0</td></tr><tr><td>Dolly-V2-12B</td><td>3.28</td><td>850</td><td>25.7</td><td>MIT</td></tr><tr><td>FastChat-T5-3B</td><td>3.04</td><td>897</td><td>47.7</td><td>Apache2.0</td></tr><tr><td>StableLM-Tuned-Alpha-7B</td><td>2.75</td><td>871</td><td>24.4</td><td>CC-BY-NC-SA-4.0</td></tr><tr><td>LLaMA-13B</td><td>2.61</td><td>826</td><td>47.0</td><td>非商用</td></tr></tbody></table><p>移动端可以看图：</p><p><img src="'+o+'" alt=""></p><p>这次更新引入了三个指标。</p><h3 id="chatbot-arena-elo" tabindex="-1">Chatbot Arena Elo <a class="header-anchor" href="#chatbot-arena-elo" aria-label="Permalink to &quot;Chatbot Arena Elo&quot;">​</a></h3><p>这个是 Areno 一直以来的评分指标， 基于使用 Elo 评级系统从 Chatbot Arena 进行的 4.2万 匿名投票。</p><h3 id="mt-bench" tabindex="-1">MT-Bench <a class="header-anchor" href="#mt-bench" aria-label="Permalink to &quot;MT-Bench&quot;">​</a></h3><p>分数基于具有挑战性的多回合基准和 GPT-4 评分，在《Judging LLM-as-a-judge with MT-Bench and Chatbot Arena》论文中提出并验证。</p><p>论文地址： <a href="https://arxiv.org/abs/2306.05685" target="_blank" rel="noreferrer">https://arxiv.org/abs/2306.05685</a></p><h3 id="mmlu" tabindex="-1">MMLU <a class="header-anchor" href="#mmlu" aria-label="Permalink to &quot;MMLU&quot;">​</a></h3><p>测量大规模多任务语言理解（Measuring Massive Multitask Language Understanding）， 这是一种广泛采用的基准，用来衡量文本模型的多任务准确性。</p><p>论文地址： <a href="https://arxiv.org/abs/2009.03300" target="_blank" rel="noreferrer">https://arxiv.org/abs/2009.03300</a></p><h2 id="mt-bench-1" tabindex="-1">MT-Bench <a class="header-anchor" href="#mt-bench-1" aria-label="Permalink to &quot;MT-Bench&quot;">​</a></h2><p>6 位代表性 LLM 在 8 个类别的能力比较：</p><ul><li>写作、</li><li>角色扮演、</li><li>推理、</li><li>数学、</li><li>编码、</li><li>提取、</li><li>STEM、</li><li>人文学科</li></ul><p>可以看到，最强的还是 GPT4，Claude v1 的 STEM（科学、技术、工程和管理）得分已经超越了 GPT4。 同时，数学、编码、推理能力上，GPT-4 遥遥领先。</p><p><img src="'+n+'" alt=""></p><h2 id="多轮对话能力" tabindex="-1">多轮对话能力 <a class="header-anchor" href="#多轮对话能力" aria-label="Permalink to &quot;多轮对话能力&quot;">​</a></h2><p>下表展示了 LLM 在第一轮和第二轮对话中的 MT 基准分数：</p><table tabindex="0"><thead><tr><th>Model</th><th>第一轮</th><th>第二轮</th><th>分数差</th></tr></thead><tbody><tr><td>GPT-4</td><td>8.96</td><td>9.03</td><td>0.07</td></tr><tr><td>Claude-v1</td><td>8.15</td><td>7.65</td><td>-0.50</td></tr><tr><td>GPT-3.5-turbo</td><td>8.08</td><td>7.81</td><td>-0.26</td></tr><tr><td>Vicuna-33B</td><td>7.46</td><td>6.79</td><td>-0.67</td></tr><tr><td>WizardLM-30B</td><td>7.13</td><td>6.89</td><td>-0.24</td></tr><tr><td>WizardLM-13B</td><td>7.12</td><td>5.59</td><td>-1.53</td></tr><tr><td>Guanaco-33B</td><td>6.88</td><td>6.18</td><td>-0.71</td></tr><tr><td>Vicuna-13B</td><td>6.81</td><td>5.96</td><td>-0.85</td></tr><tr><td>PaLM2-Chat-Bison</td><td>6.71</td><td>6.09</td><td>-0.63</td></tr><tr><td>Vicuna-7B</td><td>6.69</td><td>5.30</td><td>-1.39</td></tr><tr><td>Koala-13B</td><td>6.08</td><td>4.63</td><td>-1.45</td></tr><tr><td>MPT-7B-Chat</td><td>5.85</td><td>4.99</td><td>-0.86</td></tr><tr><td>Falcon-40B-instruct</td><td>5.81</td><td>4.53</td><td>-1.29</td></tr><tr><td>H2OGPT-Oasst-Open-LLaMA-13B</td><td>5.51</td><td>3.74</td><td>-1.78</td></tr></tbody></table><ul><li>对于<strong>开放模型</strong>，从第一轮到第二轮性能<strong>显着下降</strong>（例如Vicuna-7B、WizardLM-13B），</li><li>而强大的<strong>专有模型</strong>保持一致性。</li><li>基于 LLaMA 的模型与具有宽松许可证的模型 （MPT-7B、Falcon-40B 和指令调整的 Open-LLaMA）之间存在相当大的性能差距。</li></ul><hr><div style="text-align:center;color:#00000099;font-size:14px;">END</div>',25)]))}const M=d(l,[["render",i]]);export{u as __pageData,M as default};
