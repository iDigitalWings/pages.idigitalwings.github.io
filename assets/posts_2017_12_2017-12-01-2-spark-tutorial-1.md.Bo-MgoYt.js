import{_ as a,a as i,af as n,o as p}from"./chunks/framework.C87LdZyP.js";const d=JSON.parse('{"title":"Spark Tutorial 1","description":"","frontmatter":{"title":"Spark Tutorial 1","date":"2017-12-01T00:00:00.000Z","tags":["scala","spark"]},"headers":[],"relativePath":"posts/2017/12/2017-12-01-2-spark-tutorial-1.md","filePath":"posts/2017/12/2017-12-01-2-spark-tutorial-1.md","lastUpdated":1718187682000}'),e={name:"posts/2017/12/2017-12-01-2-spark-tutorial-1.md"};function l(t,s,k,h,r,o){return p(),i("div",null,s[0]||(s[0]=[n(`<p>这里我使用熟悉的groovy来构建项目。</p><h2 id="build-gradle" tabindex="-1"><code>build.gradle</code> <a class="header-anchor" href="#build-gradle" aria-label="Permalink to &quot;\`build.gradle\`&quot;">​</a></h2><div class="language-groovy vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">groovy</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">group </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;com.shuyi&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">version </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;1.0-SNAPSHOT&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">apply </span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">plugin</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;groovy&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">apply </span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">plugin</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;java&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">apply </span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">plugin</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;scala&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">apply </span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">plugin</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;idea&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">sourceCompatibility </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 1.8</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">repositories {</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    mavenLocal()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    maven { url </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;</span><span style="--shiki-light:#032F62;--shiki-dark:#F47067;">\${</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">nexusUrl</span><span style="--shiki-light:#032F62;--shiki-dark:#F47067;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">/content/groups/public/&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">}</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">dependencies {</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    compile </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;org.codehaus.groovy:groovy-all:2.4.13&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">            &quot;org.apache.spark:spark-core_</span><span style="--shiki-light:#032F62;--shiki-dark:#F47067;">\${</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">scalaVersion</span><span style="--shiki-light:#032F62;--shiki-dark:#F47067;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#F47067;">\${</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">sparkVersion</span><span style="--shiki-light:#032F62;--shiki-dark:#F47067;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">            &quot;org.apache.spark:spark-sql_</span><span style="--shiki-light:#032F62;--shiki-dark:#F47067;">\${</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">scalaVersion</span><span style="--shiki-light:#032F62;--shiki-dark:#F47067;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">:</span><span style="--shiki-light:#032F62;--shiki-dark:#F47067;">\${</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">sparkVersion</span><span style="--shiki-light:#032F62;--shiki-dark:#F47067;">}</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;">//            &quot;org.apache.spark:spark-mllib_\${scalaVersion}:\${sparkVersion}&quot;,</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;">//            &quot;org.apache.spark:spark-streaming_\${scalaVersion}:\${sparkVersion}&quot;,</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;">//            &quot;org.apache.spark:spark-hive_\${scalaVersion}:\${sparkVersion}&quot;,</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;">//            &quot;org.apache.spark:spark-graphx_\${scalaVersion}:\${sparkVersion}&quot;,</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;">//            &quot;org.apache.hadoop:hadoop-client:2.7.3&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    testCompile </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;junit:junit:4.12&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">}</span></span></code></pre></div><h2 id="gradle-properties" tabindex="-1"><code>gradle.properties</code> <a class="header-anchor" href="#gradle-properties" aria-label="Permalink to &quot;\`gradle.properties\`&quot;">​</a></h2><div class="language-properties vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">properties</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">sparkVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">=2.2.0</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">scalaVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">=2.11</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">scalaFullVersion</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">=2.11.8</span></span></code></pre></div><h2 id="src-main-scala-simpleapp-scala" tabindex="-1"><code>src/main/scala/SimpleApp.scala</code> <a class="header-anchor" href="#src-main-scala-simpleapp-scala" aria-label="Permalink to &quot;\`src/main/scala/SimpleApp.scala\`&quot;">​</a></h2><div class="language-scala vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">scala</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">import</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;"> org</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">apache</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">spark</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">sql</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">SparkSession</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">object</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;"> SimpleApp</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> {</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">  def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#DCBDFB;"> main</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">args</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">Array</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">[</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">String</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">]) {</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">    val</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;"> logFile</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> =</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &quot;YOUR_SPARK_HOME/README.md&quot;</span><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"> // Should be some file on your system</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">    val</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;"> spark</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> =</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;"> SparkSession</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">.builder.appName(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;Simple Application&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">).getOrCreate()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">    val</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;"> logData</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> spark.read.textFile(logFile).cache()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">    val</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;"> numAs</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> logData.filter(line </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> line.contains(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;a&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)).count()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">    val</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;"> numBs</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> logData.filter(line </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> line.contains(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;b&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)).count()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    println(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">s</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;Lines with a: </span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">$numAs</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">, Lines with b: </span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">$numBs</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    spark.stop()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">  }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">}</span></span></code></pre></div><ul><li>这里使用了<code>main</code>方法，因为使用<code>scala.App</code>的子类会产生一些问题。</li><li><code>SparkSession</code>是SparkContext，HiveContext等的一个封装</li><li><code>getOrCreate</code>方法生成SparkSession实例</li><li>然而上面的方法需要提交到spark的集群上才能运行</li></ul><p>我们现在手动创建一个<code>SparkContext</code>，</p><div class="language-scala vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">scala</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">val</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;"> conf</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;"> SparkConf</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">().setAppName(appName).setMaster(master)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;"> SparkContext</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">(conf)</span></span></code></pre></div><ul><li>appName 会显示在 cluster UI 上</li><li>master 可以是 Spark, Mesos or YARN cluster URL <ul><li>本地运行时可以传 <code>local</code>, local[x] x表示启动的线程数</li><li>实际运行时通过spark-submit，而不需要传递该参数</li></ul></li></ul><p>完整的单机版程序如下：</p><div class="language-scala vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">scala</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">import</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;"> org</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">apache</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">spark</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">.{</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">SparkConf</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">SparkContext</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">}</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">object</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;"> SimpleAppLocal</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> {</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">  def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#DCBDFB;"> main</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">args</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">: </span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">Array</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">[</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">String</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">])</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">:</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;"> Unit</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> {</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">    val</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;"> master</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> =</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &quot;local[*]&quot;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">    val</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;"> appName</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> =</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &quot;spark&quot;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">    val</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;"> conf</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;"> SparkConf</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">().setAppName(appName).setMaster(master)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">    val</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;"> spark</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;"> SparkContext</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">(conf)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">    val</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;"> file</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> =</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &quot;SimpleAppLocal.scala&quot;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">    val</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;"> logData</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> spark.textFile(file).cache()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">    val</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;"> numAs</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> logData.filter(line </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> line.contains(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;a&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)).count()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">    val</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;"> numBs</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> logData.filter(line </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> line.contains(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;b&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)).count()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    println(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">s</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;Lines with a: </span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">$numAs</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">, Lines with b: </span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">$numBs</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    spark.stop()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">  }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">}</span></span></code></pre></div><p>系统会输出：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span>Lines with a: 13, Lines with b: 3</span></span></code></pre></div><p>日志完整展示了spark的运行过程：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span>Using Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties</span></span>
<span class="line"><span>17/12/02 23:44:52 INFO SparkContext: Running Spark version 2.2.0</span></span>
<span class="line"><span>17/12/02 23:44:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span></span>
<span class="line"><span>17/12/02 23:44:58 INFO SparkContext: Submitted application: spark</span></span>
<span class="line"><span>17/12/02 23:44:58 INFO SecurityManager: Changing view acls to: alan</span></span>
<span class="line"><span>17/12/02 23:44:58 INFO SecurityManager: Changing modify acls to: alan</span></span>
<span class="line"><span>17/12/02 23:44:58 INFO SecurityManager: Changing view acls groups to: </span></span>
<span class="line"><span>17/12/02 23:44:58 INFO SecurityManager: Changing modify acls groups to: </span></span>
<span class="line"><span>17/12/02 23:44:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(alan); groups with view permissions: Set(); users  with modify permissions: Set(alan); groups with modify permissions: Set()</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO Utils: Successfully started service &#39;sparkDriver&#39; on port 53672.</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO SparkEnv: Registering MapOutputTracker</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO SparkEnv: Registering BlockManagerMaster</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO DiskBlockManager: Created local directory at /private/var/folders/w9/ktmgj7193rn9nqlwgmnl9slr0000gn/T/blockmgr-bc2a1866-0885-40af-91eb-6414c93f64ab</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO MemoryStore: MemoryStore started with capacity 2.2 GB</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO SparkEnv: Registering OutputCommitCoordinator</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO Utils: Successfully started service &#39;SparkUI&#39; on port 4040.</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.102:4040</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO Executor: Starting executor ID driver on host localhost</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO Utils: Successfully started service &#39;org.apache.spark.network.netty.NettyBlockTransferService&#39; on port 53673.</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO NettyBlockTransferService: Server created on 192.168.1.102:53673</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.102, 53673, None)</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.102:53673 with 2.2 GB RAM, BlockManagerId(driver, 192.168.1.102, 53673, None)</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.102, 53673, None)</span></span>
<span class="line"><span>17/12/02 23:44:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.102, 53673, None)</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 107.1 KB, free 2.2 GB)</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.4 KB, free 2.2 GB)</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.102:53673 (size: 20.4 KB, free: 2.2 GB)</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO SparkContext: Created broadcast 0 from textFile at SimpleAppLocal.scala:11</span></span>
<span class="line"><span>17/12/02 23:45:00 WARN ClosureCleaner: Expected a closure; got SimpleAppLocal$$$Lambda$97/1902094533</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO FileInputFormat: Total input paths to process : 1</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO SparkContext: Starting job: count at SimpleAppLocal.scala:12</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO DAGScheduler: Got job 0 (count at SimpleAppLocal.scala:12) with 2 output partitions</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO DAGScheduler: Final stage: ResultStage 0 (count at SimpleAppLocal.scala:12)</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO DAGScheduler: Parents of final stage: List()</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO DAGScheduler: Missing parents: List()</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at SimpleAppLocal.scala:12), which has no missing parents</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.8 KB, free 2.2 GB)</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 2.2 GB)</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.102:53673 (size: 2.2 KB, free: 2.2 GB)</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1006</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at filter at SimpleAppLocal.scala:12) (first 15 tasks are for partitions Vector(0, 1))</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks</span></span>
<span class="line"><span>17/12/02 23:45:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4887 bytes)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4887 bytes)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO HadoopRDD: Input split: file:/Users/alan/workspace/spark/src/main/scala/SimpleAppLocal.scala:0+313</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO HadoopRDD: Input split: file:/Users/alan/workspace/spark/src/main/scala/SimpleAppLocal.scala:313+314</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO MemoryStore: Block rdd_1_1 stored as values in memory (estimated size 712.0 B, free 2.2 GB)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO MemoryStore: Block rdd_1_0 stored as values in memory (estimated size 824.0 B, free 2.2 GB)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO BlockManagerInfo: Added rdd_1_1 in memory on 192.168.1.102:53673 (size: 712.0 B, free: 2.2 GB)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO BlockManagerInfo: Added rdd_1_0 in memory on 192.168.1.102:53673 (size: 824.0 B, free: 2.2 GB)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1572 bytes result sent to driver</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1572 bytes result sent to driver</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 234 ms on localhost (executor driver) (1/2)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 215 ms on localhost (executor driver) (2/2)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool </span></span>
<span class="line"><span>17/12/02 23:45:01 INFO DAGScheduler: ResultStage 0 (count at SimpleAppLocal.scala:12) finished in 0.265 s</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO DAGScheduler: Job 0 finished: count at SimpleAppLocal.scala:12, took 0.429026 s</span></span>
<span class="line"><span>17/12/02 23:45:01 WARN ClosureCleaner: Expected a closure; got SimpleAppLocal$$$Lambda$105/1843397873</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO SparkContext: Starting job: count at SimpleAppLocal.scala:13</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO DAGScheduler: Got job 1 (count at SimpleAppLocal.scala:13) with 2 output partitions</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO DAGScheduler: Final stage: ResultStage 1 (count at SimpleAppLocal.scala:13)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO DAGScheduler: Parents of final stage: List()</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO DAGScheduler: Missing parents: List()</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at filter at SimpleAppLocal.scala:13), which has no missing parents</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.8 KB, free 2.2 GB)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 2.2 GB)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.102:53673 (size: 2.2 KB, free: 2.2 GB)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at filter at SimpleAppLocal.scala:13) (first 15 tasks are for partitions Vector(0, 1))</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4887 bytes)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 4887 bytes)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO BlockManager: Found block rdd_1_0 locally</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO BlockManager: Found block rdd_1_1 locally</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 832 bytes result sent to driver</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 832 bytes result sent to driver</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 15 ms on localhost (executor driver) (1/2)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 18 ms on localhost (executor driver) (2/2)</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool </span></span>
<span class="line"><span>17/12/02 23:45:01 INFO DAGScheduler: ResultStage 1 (count at SimpleAppLocal.scala:13) finished in 0.019 s</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO DAGScheduler: Job 1 finished: count at SimpleAppLocal.scala:13, took 0.041909 s</span></span>
<span class="line"><span>Lines with a: 13, Lines with b: 3</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO SparkUI: Stopped Spark web UI at http://192.168.1.102:4040</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO MemoryStore: MemoryStore cleared</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO BlockManager: BlockManager stopped</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO BlockManagerMaster: BlockManagerMaster stopped</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO SparkContext: Successfully stopped SparkContext</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO ShutdownHookManager: Shutdown hook called</span></span>
<span class="line"><span>17/12/02 23:45:01 INFO ShutdownHookManager: Deleting directory /private/var/folders/w9/ktmgj7193rn9nqlwgmnl9slr0000gn/T/spark-634f225f-d4f7-4e1e-83e1-211979ac52c1</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Process finished with exit code 0</span></span></code></pre></div><ul><li><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html" target="_blank" rel="noreferrer">Spark Programming Guide</a></li><li></li></ul>`,18)]))}const g=a(e,[["render",l]]);export{d as __pageData,g as default};
