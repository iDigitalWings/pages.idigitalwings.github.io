import{_ as a,a as t,o as e,aj as o}from"./chunks/framework.Ba_Ek9Jm.js";const i="/assets/105808173582333.DPSs5j_S.png",r="/assets/105872053866083.mu5LaM2y.png",s="/assets/105902092328125.AG-z3oNX.png",n="/assets/105920243876208.Q9iKAoXn.png",p="/assets/105931515697375.Dj1WlZX9.png",l="/assets/105960173886125.BGLGv_WS.png",h="/assets/106703210876916.9A2Ysh3E.png",c="/assets/106764808254375.Dvswefyz.png",d="/assets/107094076616875.BaMzXDUJ.png",g="/assets/107160821386291.CUEcv6wa.png",m="/assets/107224830553875.52gK6UnI.png",u="/assets/107271369615458.CcsPXsee.png",f="/assets/107377916460166.B3SqotFC.png",I=JSON.parse('{"title":"CVPR 2023 候选获奖论文都讲了什么(3/4)","description":"","frontmatter":{"title":"CVPR 2023 候选获奖论文都讲了什么(3/4)","date":"2023-04-13T00:00:00.000Z","tags":["ai","ml"],"category":["AI"]},"headers":[],"relativePath":"posts/2023/04/2023-04-13-cvpr-2023-Award-Candidates-3.md","filePath":"posts/2023/04/2023-04-13-cvpr-2023-Award-Candidates-3.md","lastUpdated":1718173059000}'),_={name:"posts/2023/04/2023-04-13-cvpr-2023-Award-Candidates-3.md"},b=o('<blockquote><p>《CVPR 2023 候选获奖论文都讲了什么》第三篇，介绍一下周四前三篇论文。</p></blockquote><h2 id="《dreambooth-fine-tuning-text-to-image-diffusion-models-for-subject-driven-generation》" tabindex="-1">《DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation》 <a class="header-anchor" href="#《dreambooth-fine-tuning-text-to-image-diffusion-models-for-subject-driven-generation》" aria-label="Permalink to &quot;《DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation》&quot;">​</a></h2><p>《DreamBooth：微调文本到图像的扩散模型以实现主题驱动的生成》</p><p>这是谷歌研究院发布的论文，一句话：AI 换脸、换手、换衣服、换眼镜、换桌子、换板凳都能搞定，这里说的换，是根据已有的主题合成到你想要的任何场景里面。</p><p>Stable Diffusion 人脸不固定的问题就这样解决了。</p><p>HuggingFace 上有教程，Github 上也有开源实现，大家有兴趣可以跑一跑。</p><p><img src="'+i+'" alt=""></p><p>给你看看官网上给出的 眼镜、背包等例子，生成的图片无缝衔接。</p><p><img src="'+r+'" alt=""></p><p>不仅可以换，还能艺术风格演绎：</p><p><img src="'+s+'" alt=""></p><p>还能用文本引导：</p><p><img src="'+n+'" alt=""></p><p>还能修改颜色等属性：</p><p><img src="'+p+'" alt=""></p><p>还能加配置，穿衣服：</p><p><img src="'+l+'" alt=""></p><h3 id="社会影响" tabindex="-1">社会影响 <a class="header-anchor" href="#社会影响" aria-label="Permalink to &quot;社会影响&quot;">​</a></h3><p>最后千万注意，不要乱搞，不然造成不好的社会影响。引用下官网的话：</p><p>该项目旨在为用户提供一个有效的工具来合成不同背景下的个人主题（动物、物体）。 虽然一般的文本到图像模型在从文本合成图像时可能会偏向于特定属性， 但我们的方法使用户能够更好地重建他们想要的主题。相反，恶意方可能会尝试使用此类图像来误导观众。 这是其他生成模型方法或内容操作技术中存在的常见问题。 生成模型的未来研究，特别是个性化生成先验的研究，必须继续调查和重新验证这些问题。</p><p>论文地址：</p><ul><li><a href="https://arxiv.org/abs/2208.12242" target="_blank" rel="noreferrer">https://arxiv.org/abs/2208.12242</a> 开源实现：</li><li><a href="https://github.com/XavierXiao/Dreambooth-Stable-Diffusion" target="_blank" rel="noreferrer">https://github.com/XavierXiao/Dreambooth-Stable-Diffusion</a> 教程：</li><li><a href="https://huggingface.co/docs/diffusers/training/dreambooth" target="_blank" rel="noreferrer">https://huggingface.co/docs/diffusers/training/dreambooth</a></li></ul><h2 id="《on-distillation-of-guided-diffusion-models》" tabindex="-1">《On Distillation of Guided Diffusion Models》 <a class="header-anchor" href="#《on-distillation-of-guided-diffusion-models》" aria-label="Permalink to &quot;《On Distillation of Guided Diffusion Models》&quot;">​</a></h2><p>《关于引导扩散模型的净化》</p><p>一句话：生成图片速度更快。</p><p>多块？论文说：「与 ImageNet 上的现有方法相比，推理速度至少提高 10 倍256x256 和 LAION 数据集」。</p><p>以前10秒一张图，现在一秒一张！！！</p><h3 id="摘要" tabindex="-1">摘要 <a class="header-anchor" href="#摘要" aria-label="Permalink to &quot;摘要&quot;">​</a></h3><p>给大家看下论文摘要吧：</p><p>无分类器引导扩散模型最近被证明在高分辨率图像生成方面非常有效，并且已广泛应用于包括 DALLE-2、Stable Diffusion 和 Imagen 在内的大规模扩散框架。然而，无分类器引导扩散模型的一个缺点是，它们在推理时的计算成本很高，因为它们需要评估两个扩散模型（一个类条件模型和一个无条件模型）数十到数百次。为了解决这个限制，我们提出了一种将无分类器引导扩散模型提炼为快速采样的模型的方法：给定一个预先训练的无分类器引导模型，我们首先学习一个模型来匹配组合条件和无条件模型，然后我们逐渐将该模型提炼为需要更少采样步骤的扩散模型。对于在像素空间上训练的标准扩散模型，我们的方法能够在 ImageNet 64x64 和 CIFAR-10 上使用少至 4 个采样步骤生成视觉上与原始模型相当的图像，实现与原始模型的采样速度提高了 256 倍。对于在潜在空间上训练的扩散模型（例如，稳定扩散），我们的方法能够使用少至 1 到 4 个去噪步骤生成高保真图像，与 ImageNet 上的现有方法相比，推理速度至少提高 10 倍256x256 和 LAION 数据集。我们进一步证明了我们的方法在文本引导图像编辑和修复方面的有效性，</p><p>论文地址：</p><ul><li><a href="https://arxiv.org/abs/2210.03142" target="_blank" rel="noreferrer">https://arxiv.org/abs/2210.03142</a></li></ul><h2 id="《visual-programming-for-compositional-visual-reasoning》" tabindex="-1">《Visual Programming for Compositional Visual Reasoning》 <a class="header-anchor" href="#《visual-programming-for-compositional-visual-reasoning》" aria-label="Permalink to &quot;《Visual Programming for Compositional Visual Reasoning》&quot;">​</a></h2><p>《用于组合视觉推理的可视化编程》</p><p>一句话：让设计师和 P 图师失业的神器。</p><p><img src="'+h+'" alt=""></p><p>一种神经符号系统，可在给定自然语言指令的情况下解决复杂的组合视觉任务。VisProg 使用 GPT3 的上下文学习能力生成 python 程序，然后执行这些程序以获得解决方案和全面且可解释的基本原理。生成的程序的每一行都可以调用几个现成的计算机视觉模型、图像处理例程或 python 函数中的一个，以生成可能被程序的后续部分使用的中间输出。</p><p>来看看他怎么实现用自然语言来进行图片干编辑：</p><p><img src="'+c+'" alt="『把背景换成雪地和极地白熊』"></p><p>这他喵的以后 P图的设计师彻底失业了。</p><p>再分享一些例子给搭建看看他的神奇之处。</p><h3 id="把莱昂纳多·迪卡普里奥换成戴墨镜的莱昂纳多·迪卡普里奥" tabindex="-1">把莱昂纳多·迪卡普里奥换成戴墨镜的莱昂纳多·迪卡普里奥 <a class="header-anchor" href="#把莱昂纳多·迪卡普里奥换成戴墨镜的莱昂纳多·迪卡普里奥" aria-label="Permalink to &quot;把莱昂纳多·迪卡普里奥换成戴墨镜的莱昂纳多·迪卡普里奥&quot;">​</a></h3><p><img src="'+d+'" alt=""></p><h3 id="把沙漠换成海滩" tabindex="-1">把沙漠换成海滩 <a class="header-anchor" href="#把沙漠换成海滩" aria-label="Permalink to &quot;把沙漠换成海滩&quot;">​</a></h3><p><img src="'+g+'" alt=""></p><h3 id="把沙发换成毛绒蓝沙发" tabindex="-1">把沙发换成毛绒蓝沙发 <a class="header-anchor" href="#把沙发换成毛绒蓝沙发" aria-label="Permalink to &quot;把沙发换成毛绒蓝沙发&quot;">​</a></h3><p><img src="'+m+'" alt=""></p><h3 id="标记德国、台湾和新西兰的女性领导人" tabindex="-1">标记德国、台湾和新西兰的女性领导人 <a class="header-anchor" href="#标记德国、台湾和新西兰的女性领导人" aria-label="Permalink to &quot;标记德国、台湾和新西兰的女性领导人&quot;">​</a></h3><p>是不是感觉电影中的人工智能马上到来了。</p><p><img src="'+u+'" alt=""></p><h3 id="给《戴珍珠耳环的女孩》画上标签" tabindex="-1">给《戴珍珠耳环的女孩》画上标签 <a class="header-anchor" href="#给《戴珍珠耳环的女孩》画上标签" aria-label="Permalink to &quot;给《戴珍珠耳环的女孩》画上标签&quot;">​</a></h3><p><img src="'+f+'" alt=""></p><h3 id="项目地址" tabindex="-1">项目地址 <a class="header-anchor" href="#项目地址" aria-label="Permalink to &quot;项目地址&quot;">​</a></h3><p>项目地址：</p><ul><li><a href="https://prior.allenai.org/projects/visprog" target="_blank" rel="noreferrer">https://prior.allenai.org/projects/visprog</a> 代码：</li><li><a href="https://github.com/allenai/visprog" target="_blank" rel="noreferrer">https://github.com/allenai/visprog</a></li></ul>',55),v=[b];function P(q,x,D,k,A,C){return e(),t("div",null,v)}const T=a(_,[["render",P]]);export{I as __pageData,T as default};
