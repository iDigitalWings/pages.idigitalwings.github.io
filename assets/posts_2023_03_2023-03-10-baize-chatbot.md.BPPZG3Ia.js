import{_ as a,a as i,af as t,o as e}from"./chunks/framework.C87LdZyP.js";const l="/assets/483736439353416.CdjkncYe.png",h="/assets/483895077595666.DtE9Xm0M.png",F=JSON.parse('{"title":"baize-chatbot: 让 ChatGPT 使用单个 GPU 在数小时内教会您自己的聊天机器人！","description":"","frontmatter":{"title":"baize-chatbot: 让 ChatGPT 使用单个 GPU 在数小时内教会您自己的聊天机器人！","date":"2023-03-10T00:00:00.000Z","tags":["ai","ml"],"category":["AI"]},"headers":[],"relativePath":"posts/2023/03/2023-03-10-baize-chatbot.md","filePath":"posts/2023/03/2023-03-10-baize-chatbot.md","lastUpdated":1718173059000}'),n={name:"posts/2023/03/2023-03-10-baize-chatbot.md"};function p(k,s,d,r,o,c){return e(),i("div",null,s[0]||(s[0]=[t('<blockquote><p>Baize 是一种使用LoRA训练的开源聊天模型。 它使用 ChatGPT 与自身聊天生成的 100k 对话，同时使用了 Alpaca 的数据来提高其性能。 目前已经发布了 7B、13B 和 30B 型号。你可以根据官方的指导来自己训练自己的数据， 或者使用 LoRA 来进行微调。</p></blockquote><h2 id="预览" tabindex="-1">预览 <a class="header-anchor" href="#预览" aria-label="Permalink to &quot;预览&quot;">​</a></h2><p><img src="'+l+'" alt=""></p><p>注意：该模型对中文的支持不是很好，虽然能听得懂，但是让它用中文回答的话，问题比较多。</p><p><img src="'+h+`" alt=""></p><h2 id="为什么叫白泽呢" tabindex="-1">为什么叫白泽呢？ <a class="header-anchor" href="#为什么叫白泽呢" aria-label="Permalink to &quot;为什么叫白泽呢？&quot;">​</a></h2><p>白泽（发音为By-zor；简体中文白泽，繁体中文白泽，日文白泽，はくたく）是中国民间传说中的神话生物，会说人类语言，无所不知。这正是我们对聊天模型的期望。</p><h2 id="安装" tabindex="-1">安装 <a class="header-anchor" href="#安装" aria-label="Permalink to &quot;安装&quot;">​</a></h2><p>首先，安装最新版本的 <strong>Fastchat</strong>：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> git+https://github.com/huggingface/peft.git</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> git+https://github.com/lm-sys/FastChat.git</span></span></code></pre></div><p>（仅适用于 v1 模型）：将 Baize 的 LoRA 权重合并到 LLaMA 中。以7B关卡为例。</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># Note you have to include &quot;baize&quot; in the target directory so Fastchat can recognize Baize.</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">python3</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> fastchat.model.apply_lora</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --base</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> huggyllama/llama-7b</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --target</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ./model_weights/baize-7b</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --lora</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> project-baize/baize-lora-7B</span></span></code></pre></div><p>现在，在您的终端中运行 CLI！更多选项和配置可以在这里找到。</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># Optional: Add \`--style rich\` for better style.</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">python</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> fastchat.serve.cli</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --model-path</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ./model_weights/baize-7b</span></span></code></pre></div><p>您可以按照此处的说明将 <strong>Baize</strong> 与 OpenAI API 或 Hugging Face API 结合使用。</p><h3 id="本地运行" tabindex="-1">本地运行 <a class="header-anchor" href="#本地运行" aria-label="Permalink to &quot;本地运行&quot;">​</a></h3><p>首先，确保您的 Python 版本是 3.8，然后使用以下命令安装所需的包：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">cd</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> demo</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -r</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> requirements.txt</span></span></code></pre></div><p>您可以使用以下命令在本地计算机上托管模型：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># We assume you have obtained access to use LLaMA. The following LLaMA weights are from a 3rd party.</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">base_model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">huggyllama/llama-7b</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">lora_model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">project-baize/baize-lora-7B</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> app.py</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> $base_model $lora_model</span></span></code></pre></div><p>对于 v2 模型（已经合并），只需运行：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># We assume you have obtained access to use LLaMA.</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">base_model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">project-baize/baize-v2-7b</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> app.py</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> $base_model </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">None</span></span></code></pre></div><h4 id="gpu-显存要求" tabindex="-1">GPU 显存要求 <a class="header-anchor" href="#gpu-显存要求" aria-label="Permalink to &quot;GPU 显存要求&quot;">​</a></h4><table tabindex="0"><thead><tr><th></th><th>推理（没有 int8）</th></tr></thead><tbody><tr><td>白泽7B</td><td>16 GB</td></tr><tr><td>白泽13B</td><td>28GB</td></tr><tr><td>白泽30B</td><td>67GB</td></tr></tbody></table><p>如果您的 GPU 具有较小的 VRAM，则可以<code>int8</code>通过传递 8 位参数来使用 , 进行推理：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> app.py</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> $base_model $lora_model </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">8bit</span></span></code></pre></div><h2 id="训练自己的模型" tabindex="-1">训练自己的模型 <a class="header-anchor" href="#训练自己的模型" aria-label="Permalink to &quot;训练自己的模型&quot;">​</a></h2><h3 id="设置" tabindex="-1">设置 <a class="header-anchor" href="#设置" aria-label="Permalink to &quot;设置&quot;">​</a></h3><ol><li>安装依赖</li></ol><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -r</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> requirements.txt</span></span></code></pre></div><ol start="2"><li>如果<code>bitsandbytes</code>不起作用，<a href="https://github.com/TimDettmers/bitsandbytes/blob/main/compile_from_source.md" target="_blank" rel="noreferrer">请从源安装它</a>。Windows 用户可以按照<a href="https://github.com/tloen/alpaca-lora/issues/17" target="_blank" rel="noreferrer">这些说明</a>进行操作。</li></ol><h3 id="资料收集" tabindex="-1">资料收集 <a class="header-anchor" href="#资料收集" aria-label="Permalink to &quot;资料收集&quot;">​</a></h3><p>您可以使用我们<a href="https://github.com/project-baize/baize-chatbot/blob/main/data" target="_blank" rel="noreferrer">发布的数据</a>或使用以下命令从 ChatGPT 收集数据：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">num_process</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">10</span><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"> # The number of processes to collect data</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">max_total_tokens</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">500000</span><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"> # Set maximum numbers of tokens to collect data</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">api_key</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">xxxxxxxxxxxxxxxxx</span><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"> # Set your openai api key</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> ((i</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">; i</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">&lt;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">$num_process; i</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">++</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">))</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">do</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">    python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> collect.py</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> $api_key $max_total_tokens $i $num_process </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">stackoverflow</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> &amp;</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">    python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> collect.py</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> $api_key $max_total_tokens $i $num_process </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">quora</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> &amp;</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">    python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> collect.py</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> $api_key $max_total_tokens $i $num_process </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">medical</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> &amp;</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">done</span></span></code></pre></div><p>收集数据后，使用以下命令对数据进行预处理：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> preprocess.py</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> stackoverflow</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> preprocess.py</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> quora</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> preprocess.py</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> medical</span></span></code></pre></div><h3 id="使用您自己的数据" tabindex="-1">使用您自己的数据 <a class="header-anchor" href="#使用您自己的数据" aria-label="Permalink to &quot;使用您自己的数据&quot;">​</a></h3><p>如果您想将特定数据集用作 ChatGPT 自我聊天的种子，您只需修改即可<code>collect.py</code>加载自己的数据。</p><h3 id="训练" tabindex="-1">训练 <a class="header-anchor" href="#训练" aria-label="Permalink to &quot;训练&quot;">​</a></h3><p>微调代码旨在在 A100-80G GPU 上运行。该<code>finetune.py</code>脚本接受三个参数：基础模型大小（即 7B、13B 或 30B）、批量大小、学习率和数据集。<a href="https://github.com/project-baize/baize/blob/cbcf39902fcdfab8d935b7ea771a4e7d452a1be0/finetune.py#L24" target="_blank" rel="noreferrer">请注意，总批大小固定为 64（可在此处</a>修改），此处的批大小是梯度累积前每个设备的批大小。如果您在 VRAM 较小的 GPU 上进行训练，请将其设置为较小的值。</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># For the 7B model (takes about 9 hours)</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> finetune.py</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> 7b</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 32</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 0.0002</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> alpaca,stackoverflow,quora</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># For the 13B model (takes about 16 hours)</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> finetune.py</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> 13b</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 16</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 0.0001</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> alpaca,stackoverflow,quora</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># For the 30B model (takes about 36 hours)</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> finetune.py</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> 30b</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 8</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 0.00005</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> alpaca,stackoverflow,quora</span></span></code></pre></div><h4 id="gpu-显存消耗" tabindex="-1">GPU 显存消耗 <a class="header-anchor" href="#gpu-显存消耗" aria-label="Permalink to &quot;GPU 显存消耗&quot;">​</a></h4><p>使用上面的设置：</p><table tabindex="0"><thead><tr><th></th><th>培训（使用 int8）</th></tr></thead><tbody><tr><td>白泽7B</td><td>26GB</td></tr><tr><td>白泽13B</td><td>25GB</td></tr><tr><td>白泽30B</td><td>42GB</td></tr></tbody></table><p>有问题吗？看到<a href="https://github.com/project-baize/baize-chatbot/issues/26" target="_blank" rel="noreferrer">这个问题</a>。</p><h3 id="将-lora-合并到-llama" tabindex="-1">将 LoRA 合并到 LLaMA <a class="header-anchor" href="#将-lora-合并到-llama" aria-label="Permalink to &quot;将 LoRA 合并到 LLaMA&quot;">​</a></h3><p>现在您可以轻松地将经过训练的 LoRA 权重合并到 LLaMA 模型中，这样您就可以将它与支持标准 Hugging Face API 的所有东西一起使用！</p><p>这是合并到 LLaMA-7B 的示例<code>baize-lora-7B</code>。</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> merge_lora.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">--base </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">huggyllama/llama-7b</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">--target </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">~/model_weights/baize-7b</span><span style="--shiki-light:#005CC5;--shiki-dark:#F47067;"> \\</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">--lora </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">project-baize/baize-lora-7B</span></span></code></pre></div><h2 id="项目" tabindex="-1">项目 <a class="header-anchor" href="#项目" aria-label="Permalink to &quot;项目&quot;">​</a></h2><p>项目地址：</p><ul><li><a href="https://github.com/project-baize/baize-chatbot" target="_blank" rel="noreferrer">https://github.com/project-baize/baize-chatbot</a></li></ul><hr><div style="text-align:center;color:#00000099;font-size:14px;">END</div>`,54)]))}const y=a(n,[["render",p]]);export{F as __pageData,y as default};
