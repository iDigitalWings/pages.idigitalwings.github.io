import{_ as s}from"./chunks/1983886111973000.C5OmKY3L.js";import{_ as a,a as i,o as t,aj as e}from"./chunks/framework.Ba_Ek9Jm.js";const n="/assets/1976073278993250.cHfMtRqR.png",l="/assets/2010765911278458.CsflHq09.png",p="/assets/2011377909164583.B481tHUQ.png",h="/assets/2011721841688958.BAtJmY_M.png",o="/assets/2011968148639916.Df8oDnJY.png",r="/assets/1980210207436375.DSaE9JAK.png",d="/assets/1982107753740541.CDcPA3z2.png",k="/assets/1982515427462583.GroTXB-c.png",F=JSON.parse('{"title":"LLM 数据架构《Llama Index》：专注大型语言模型数据层的解决方案","description":"","frontmatter":{"title":"LLM 数据架构《Llama Index》：专注大型语言模型数据层的解决方案","date":"2023-06-12T00:00:00.000Z","tags":["ai","ml"],"category":["AI"]},"headers":[],"relativePath":"posts/2023/06/2023-06-12-llama-index-intro.md","filePath":"posts/2023/06/2023-06-12-llama-index-intro.md","lastUpdated":1718173059000}'),c={name:"posts/2023/06/2023-06-12-llama-index-intro.md"},g=e('<blockquote><p>在之前查看 MongoDB LLM 加载器的时候，搜到这个这个框架，这几天看了一下，真是一个<strong>宝藏框架</strong>。 企业内部做 LLM 开发大多是用于企业内部数据，而且是五花八门的数据，LangChain 等框架用着用着 就会发现有点儿力不从心。</p><p>Llama Index 提供了现阶段几乎完美的解决方案。</p></blockquote><p>今天先介绍下 LlamaIndex 项目基本信息、基本用户、项目的高级概念，后续再详细介绍项目的牛逼之处。</p><h2 id="llamaindex-介绍" tabindex="-1">LlamaIndex 介绍 <a class="header-anchor" href="#llamaindex-介绍" aria-label="Permalink to &quot;LlamaIndex 介绍&quot;">​</a></h2><p>LlamaIndex 是一个『数据框架』，可帮助您构建 LLM 应用程序。它提供了以下工具：</p><ul><li>提供<strong>数据连接器</strong>来摄取您现有的数据源和数据格式（API、PDF、文档、SQL 等）</li><li>提供<strong>构建数据</strong>（索引、图表）的方法，以便LLM可以轻松使用该数据。</li><li><strong>引擎</strong>提供对数据的自然语言访问。例如： <ul><li>查询引擎是用于知识增强输出的强大检索接口。</li><li>聊天引擎是用于与数据进行多消息、“来回”交互的对话界面。</li></ul></li><li><strong>数据代理</strong>是由 LLM 提供支持的知识工作者，并通过工具进行增强，从简单的辅助函数到 API 集成等。</li><li>为您的数据提供<strong>高级检索/查询界面</strong>：输入任何 LLM 输入提示，获取检索到的上下文和知识增强输出。</li><li>允许与<strong>外部应用程序框架轻松集成</strong>（例如与 LangChain、Flask、Docker、ChatGPT 等）。</li></ul><p><img src="'+n+'" alt=""></p><p>LlamaIndex 为初学者和高级用户提供工具。我们的高级 API 允许初学者使用 LlamaIndex 通过 5 行代码获取和查询他们的数据。 我们的低级 API 允许高级用户自定义和扩展任何模块（数据连接器、索引、检索器、查询引擎、重新排名模块），以满足他们的需求。</p><p><img src="'+l+'" alt="完整查询架构"></p><p>LlamaIndex 提供了用数据增强 LLM 关键工具：</p><p><img src="'+p+'" alt=""></p><h3 id="轻松构建强大的最终用户应用程序" tabindex="-1">轻松构建强大的最终用户应用程序 <a class="header-anchor" href="#轻松构建强大的最终用户应用程序" aria-label="Permalink to &quot;轻松构建强大的最终用户应用程序&quot;">​</a></h3><p><img src="'+h+'" alt=""></p><h3 id="连接非结构化、结构化或半结构化数据源" tabindex="-1">连接非结构化、结构化或半结构化数据源 <a class="header-anchor" href="#连接非结构化、结构化或半结构化数据源" aria-label="Permalink to &quot;连接非结构化、结构化或半结构化数据源&quot;">​</a></h3><p><img src="'+o+'" alt=""></p><h2 id="安装" tabindex="-1">安装 <a class="header-anchor" href="#安装" aria-label="Permalink to &quot;安装&quot;">​</a></h2><p>使用 pip 安装：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> llama-index</span></span></code></pre></div><blockquote><p>LlamaIndex 可能会下载并存储各种软件包的本地文件（NLTK、HuggingFace 等）。 使用环境变量 <code>LLAMA_INDEX_CACHE_DIR</code> 来控制这些文件的保存位置。</p></blockquote><h2 id="llm-设置" tabindex="-1">LLM 设置 <a class="header-anchor" href="#llm-设置" aria-label="Permalink to &quot;LLM 设置&quot;">​</a></h2><p>你可以使用 OpenAI 或者其他的 <strong>LLM 厂商</strong> 的服务来进行嵌入和文本生成， 当然你也可以使用<strong>本地运行LLM模型</strong> 来完成文本生成和嵌入。</p><h3 id="使用-openai" tabindex="-1">使用 OpenAI <a class="header-anchor" href="#使用-openai" aria-label="Permalink to &quot;使用 OpenAI&quot;">​</a></h3><p>默认情况下，LlamaIndex 使用 OpenAI 的 <code>gpt-3.5-turbo</code>模型进行文本生成 以及<code>text-embedding-ada-002</code>来进行检索和嵌入。只需要设置环境变量 <code>OPENAI_API_KEY</code> 即可。</p><h3 id="使用本地环境" tabindex="-1">使用本地环境 <a class="header-anchor" href="#使用本地环境" aria-label="Permalink to &quot;使用本地环境&quot;">​</a></h3><p>不想使用 OpenAI，环境将自动回退到使用 <code>LlamaCPP</code> 和 <code>llama2-chat-13B</code> 进行文本生成， 以及使用 <code>BAAI/bge-small-en</code> 进行检索和嵌入，这些模型都将在本地运行（自动从网上下载）。</p><p>安装依赖库：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> sentence-transformers</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> llama-cpp-python</span></span></code></pre></div><h4 id="准备数据文件" tabindex="-1">准备数据文件 <a class="header-anchor" href="#准备数据文件" aria-label="Permalink to &quot;准备数据文件&quot;">​</a></h4><p>我这里准备了关于杭州亚运会奖牌的一篇文章：</p><p><img src="'+r+`" alt=""></p><p>使用 llama_index 加载文件夹，并构建索引：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> llama_index </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> VectorStoreIndex, SimpleDirectoryReader</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">documents </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> SimpleDirectoryReader(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;亚运会&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">).load_data()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">index </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> VectorStoreIndex.from_documents(documents)</span></span></code></pre></div><p>运行查询，问问他：<code>杭州亚运会奖牌的工艺和设计理念?</code></p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">query_engine </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> index.as_query_engine()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">response </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> query_engine.query(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;杭州亚运会奖牌的工艺和设计理念?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">(response)</span></span></code></pre></div><p>输出的结果为：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span>杭州亚运会奖牌的工艺是江南的传统工艺——打鏨雕，</span></span>
<span class="line"><span>背面则采用了杭州江南丝绣的技艺。</span></span>
<span class="line"><span>奖牌的设计理念是将杭州的城市元素与杭州丝绸的优势相结合，</span></span>
<span class="line"><span>表达独特的美学理念，</span></span>
<span class="line"><span>并将深厚底蕴的南宋文化与杭州地域特色相融合。</span></span></code></pre></div><p>效果还是不错的。</p><h3 id="保存和加载数据" tabindex="-1">保存和加载数据 <a class="header-anchor" href="#保存和加载数据" aria-label="Permalink to &quot;保存和加载数据&quot;">​</a></h3><p>默认情况下，数据存储在内存中。使用如下方法保存到磁盘（默认<code>./storage</code>）：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">index.storage_context.persist()</span></span></code></pre></div><p>加载数据：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">rom llama_index </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> StorageContext, load_index_from_storage</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># rebuild storage context</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">storage_context </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> StorageContext.from_defaults(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">persist_dir</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;./storage&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># load index</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">index </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> load_index_from_storage(storage_context)</span></span></code></pre></div><p>我们增加判断，如果文件存在就加载，不存在就创建索引并保存：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">if</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> os.path.exists(join(persist_path, </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;vector_store.json&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    storage_context </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> StorageContext.from_defaults(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">persist_dir</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">persist_path)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    index </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> load_index_from_storage(storage_context)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">else</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    documents </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> SimpleDirectoryReader(doc_path).load_data()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    index </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> VectorStoreIndex.from_documents(documents)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">    index.storage_context.persist(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">persist_dir</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">persist_path)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#768390;"># ./storage</span></span></code></pre></div><p>基于前面这几行代码，我们就可以把上次用 LangChain 构件的知识库替换成 LlamaIndex 的实现，是不是很简单。</p><h2 id="高级概念" tabindex="-1">高级概念 <a class="header-anchor" href="#高级概念" aria-label="Permalink to &quot;高级概念&quot;">​</a></h2><h3 id="检索增强生成-rag" tabindex="-1">检索增强生成 (RAG) <a class="header-anchor" href="#检索增强生成-rag" aria-label="Permalink to &quot;检索增强生成 (RAG)&quot;">​</a></h3><p><strong>检索增强生成</strong> (RAG) 是使用自定义数据增强 LLM 的范例（知识库一般都是这个范例哦）。</p><p>一般由两个阶段组成：</p><ul><li><strong>索引阶段</strong>：准备知识库，以及</li><li><strong>查询阶段</strong>：从知识中检索相关上下文，以协助LLM回答问题</li></ul><p>大致如下：</p><p><img src="`+d+'" alt=""></p><h4 id="索引阶段" tabindex="-1">索引阶段 <a class="header-anchor" href="#索引阶段" aria-label="Permalink to &quot;索引阶段&quot;">​</a></h4><p>LlamaIndex 通过数据连接器和索引来完成知识库准备：</p><p><img src="'+k+'" alt=""></p><p>这里主要有三个概念：</p><p><strong>数据连接器</strong>：</p><ul><li>数据连接器（即Reader）将来自不同数据源和数据格式的数据摄取为简单的Document表示形式（文本和简单的元数据）。</li></ul><p><strong>文档/节点</strong>：</p><ul><li><code>Document</code>是任何数据源的通用容器 - 例如 PDF、API 输出或从数据库检索的数据。<code>Node</code>是 LlamaIndex 中数据的原子单位，代表源的一个“块”Document。Document 是一种丰富的表示形式，包括元数据和（与其他节点的）关系，以实现准确且富有表现力的检索操作。</li></ul><p><strong>数据索引</strong>：</p><ul><li>获取数据后，LlamaIndex 将帮助您将数据索引为易于检索的格式。在底层，LlamaIndex 将原始文档解析为中间表示、计算向量嵌入并推断元数据。最常用的索引是 <code>VectorStoreIndex</code></li></ul><h4 id="查询阶段" tabindex="-1">查询阶段 <a class="header-anchor" href="#查询阶段" aria-label="Permalink to &quot;查询阶段&quot;">​</a></h4><p>查询阶段，RAG 管道检索给定用户查询的最相关上下文， 并将其传递给 LLM（连同查询）以合成响应。 这为LLM提供了原始训练数据中没有的最新知识（也减少了幻觉）。</p><p>查询阶段的关键挑战是对（可能很多）<strong>知识库的检索、编排和推理</strong>。</p><p><img src="'+s+'" alt=""></p><h5 id="构建块" tabindex="-1">构建块 <a class="header-anchor" href="#构建块" aria-label="Permalink to &quot;构建块&quot;">​</a></h5><blockquote><p>构建部分可能用到三个模块：检索器、节点后处理器、响应合成器。</p></blockquote><p><strong>检索器</strong>：检索器定义在给定查询时如何从知识库（即索引）有效地检索相关上下文。不同索引的具体检索逻辑有所不同，最流行的是<code>针对向量索引的密集检索</code>。</p><p><strong>节点后处理器</strong>：节点后处理器接收一组节点，然后对它们应用转换、过滤或重新排序逻辑。</p><p><strong>响应合成器</strong>：响应合成器使用用户查询和给定的一组检索到的文本块从 LLM 生成响应。</p><h5 id="管道" tabindex="-1">管道 <a class="header-anchor" href="#管道" aria-label="Permalink to &quot;管道&quot;">​</a></h5><blockquote><p>常用的管道有三个，分别是 查询引擎、聊天引擎、代理。</p></blockquote><p><strong>查询引擎</strong>：查询引擎是一个端到端的管道，允许您对数据提出问题。它接受自然语言查询，并返回响应，以及检索并传递给LLM的参考上下文。</p><p><strong>聊天引擎</strong>：聊天引擎是一个端到端的管道，用于与您的数据进行对话（多次来回而不是单个问题和答案）。</p><p><strong>代理</strong>：代理是一个自动决策者（由LLM提供支持），通过一组工具与世界交互。代理可以以与查询引擎或聊天引擎相同的方式使用。主要区别在于代理动态地决定最佳的操作顺序，而不是遵循预定的逻辑。这赋予它额外的灵活性来处理更复杂的任务。</p><h2 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-label="Permalink to &quot;总结&quot;">​</a></h2><p>连接器、文档、索引以及检索器、处理器、合成器，就是这六个基本概念，后面我们一一介绍，以及怎么基于 Llama Index 来构建项目。</p><hr><ul><li><p><a href="https://github.com/arc53/DocsGPT" target="_blank" rel="noreferrer">https://github.com/arc53/DocsGPT</a></p></li><li><p><a href="https://docsgpt.arc53.com/" target="_blank" rel="noreferrer">https://docsgpt.arc53.com/</a></p></li><li><p><a href="https://www.xlang.ai/" target="_blank" rel="noreferrer">https://www.xlang.ai/</a></p></li><li><p><a href="https://github.com/taoyds/spider" target="_blank" rel="noreferrer">https://github.com/taoyds/spider</a></p></li><li><p><a href="https://paperswithcode.com/sota/text-to-sql-on-spider" target="_blank" rel="noreferrer">https://paperswithcode.com/sota/text-to-sql-on-spider</a></p></li></ul><hr><div style="text-align:center;color:#00000099;font-size:14px;">END</div>',81),m=[g];function u(A,y,_,b,D,x){return t(),i("div",null,m)}const q=a(c,[["render",u]]);export{F as __pageData,q as default};
