import{_ as l,a as e,af as t,o}from"./chunks/framework.C87LdZyP.js";const r="/assets/949557379682458.BP13qLPD.png",f=JSON.parse(`{"title":"ACL'23 最佳论文：《从预训练数据到语言模型再到下游任务：追踪导致不公平 NLP 模型的政治偏见的踪迹》","description":"","frontmatter":{"title":"ACL'23 最佳论文：《从预训练数据到语言模型再到下游任务：追踪导致不公平 NLP 模型的政治偏见的踪迹》","date":"2023-05-18T00:00:00.000Z","tags":["ai，ml"],"category":["AI"]},"headers":[],"relativePath":"posts/2023/05/2023-05-18-acl23-Unfair-NLP-model.md","filePath":"posts/2023/05/2023-05-18-acl23-Unfair-NLP-model.md","lastUpdated":1718173059000}`),i={name:"posts/2023/05/2023-05-18-acl23-Unfair-NLP-model.md"};function p(n,a,s,c,d,h){return o(),e("div",null,a[0]||(a[0]=[t('<blockquote><p>第61届计算语言学协会年会（ACL&#39;23）于2023年7月9日至14日在加拿大多伦多举行，现已圆满结束， 今天带来第二篇 ACL&#39;23 获奖论文：《从预训练数据到语言模型再到下游任务：追踪导致不公平 NLP 模型的政治偏见的踪迹》。</p></blockquote><blockquote><p>公众号后台回复： <code>ACL23 656</code> 下载论文。</p></blockquote><p>同样，我还是先用一两句话解释一下这个论文讲的啥：</p><p><strong>作者提出了一种新的方法来测量语言模型中的政治偏见, 并发现预训练语料的政治极化会通过语言模型传播到下游任务中, 从而影响这些社会性任务的公平性。 论文揭示了消除预训练语料偏见对提高模型公平性的重要性。</strong></p><p>这篇论文的得奖，我感觉很明显压在了大语言模型的热点上了，那就是<code>偏见</code>，从技术男的角度看，论文并没有让人眼前一亮的东西。</p><h2 id="如何测量政治偏见" tabindex="-1">如何测量政治偏见 <a class="header-anchor" href="#如何测量政治偏见" aria-label="Permalink to &quot;如何测量政治偏见&quot;">​</a></h2><p>作者提出了一种新的方法来测量语言模型中的政治偏见,具体步骤:</p><ul><li>根据政治光谱理论,设计了包含社会和经济两个维度的政治倾向测试。</li><li>通过填空和生成后评估的方式,考察语言模型对测试语句的支持或反对程度。</li><li>根据语言模型对测试语句的反应,量化其在社会和经济两个维度上的政治倾向坐标。</li><li>比较不同语言模型和进一步预训练的语言模型的政治倾向。</li></ul><p>这种方法基于政治科学理论,可以量化语言模型中的政治偏见,是本论文的创新之处。</p><h3 id="政治光谱理论" tabindex="-1">政治光谱理论 <a class="header-anchor" href="#政治光谱理论" aria-label="Permalink to &quot;政治光谱理论&quot;">​</a></h3><p><img src="'+r+'" alt="简单画了个图，大家且看"></p><p>政治光谱理论是描述和测量个人及团体政治倾向的一个理论框架。其主要观点包括:</p><ol><li>将政治倾向区分为社会倾向(保守对立自由)和经济倾向(左对立右)两个维度。</li><li>在每个维度上,个人及团体的政治观点都处于一个连续光谱上,而不是非此即彼的两极。</li><li>通过问卷调查等方式,可以测量出个人或团体在这两个维度上的坐标位置。</li><li>个人和团体的政治倾向往往不在一个维度上高度一致,如可以在社会上保守而经济上偏左。</li></ol><p>这种理论比简单的左右区分更加细致和全面。本论文采用这一理论作为分析语言模型政治偏见的基础。</p><h2 id="现有模型的政治偏见" tabindex="-1">现有模型的政治偏见 <a class="header-anchor" href="#现有模型的政治偏见" aria-label="Permalink to &quot;现有模型的政治偏见&quot;">​</a></h2><p>论文中探究了不同预训练语言模型的政治倾向，论文提供了可重复的实证结果,表明预训练语言模型确实会吸收训练语料的政治偏见。</p><p>主要发现如下:</p><ul><li>使用政治倾向测试语句探测了包括BERT、GPT、RoBERTa等在内的14个语言模型的政治倾向。</li><li>不同语言模型占据政治坐标的不同象限,表现出明显的政治差异。例如BERT系列更保守,而GPT系列更自由。</li><li>语言模型在社会议题上的偏见更明显,比经济议题上的偏见更强。</li><li>在六个具有不同立场的预训练语料上继续预训练语言模型,发现它们会从中获取政治偏见,语料立场越极端,语言模型的坐标变化越大。</li><li>分析语料的时间维度,发现语言模型可以检测出特朗普当选后的社会两极化现象,产生更极端的偏移。</li><li>定量分析了训练更多轮次、使用更大规模语料产生“超党派”语言模型的难度。</li></ul><h2 id="微调的政治偏见" tabindex="-1">微调的政治偏见 <a class="header-anchor" href="#微调的政治偏见" aria-label="Permalink to &quot;微调的政治偏见&quot;">​</a></h2><p>论文还指出： 通过在不同立场的语料上进一步预训练语言模型,发现它们会从预训练语料中获得政治偏见。</p><p>也就是，<strong>微调大语言模型可以让其获得偏见</strong>。</p><h2 id="其他" tabindex="-1">其他 <a class="header-anchor" href="#其他" aria-label="Permalink to &quot;其他&quot;">​</a></h2><p>论文还提出集成不同政治倾向模型，使用它们的集体知识改进下游任务，以及消除政治偏见的重要性。</p><p>更多内容大家可以去下面。</p><blockquote><p>公众号后台回复： <code>ACL23 656</code> 下载论文。</p></blockquote><h2 id="资料" tabindex="-1">资料 <a class="header-anchor" href="#资料" aria-label="Permalink to &quot;资料&quot;">​</a></h2><p>论文：</p><ul><li><a href="https://arxiv.org/abs/2305.08283" target="_blank" rel="noreferrer">https://arxiv.org/abs/2305.08283</a></li><li><a href="https://aclanthology.org/2023.acl-long.656/" target="_blank" rel="noreferrer">https://aclanthology.org/2023.acl-long.656/</a></li><li><a href="https://aclanthology.org/2023.acl-long.656.pdf" target="_blank" rel="noreferrer">https://aclanthology.org/2023.acl-long.656.pdf</a></li></ul><hr><div style="text-align:center;color:#00000099;font-size:14px;">END</div>',30)]))}const g=l(i,[["render",p]]);export{f as __pageData,g as default};
