import{_ as i}from"./chunks/1397863291123583.BVaNHdVv.js";import{_ as a,a as l,af as n,o as h}from"./chunks/framework.C87LdZyP.js";const c=JSON.parse('{"title":"使用 OpenLLM 运行 ChatGLM、Llama 等任意大语言模型（LLM）","description":"","frontmatter":{"title":"使用 OpenLLM 运行 ChatGLM、Llama 等任意大语言模型（LLM）","date":"2023-05-29T00:00:00.000Z","tags":["ai","ml"],"category":["AI"]},"headers":[],"relativePath":"posts/2023/05/2023-05-30-openllm.md","filePath":"posts/2023/05/2023-05-30-openllm.md","lastUpdated":1718175555000}'),t={name:"posts/2023/05/2023-05-30-openllm.md"};function e(p,s,k,d,F,r){return h(),l("div",null,s[0]||(s[0]=[n(`<blockquote><p>今天介绍这个工具（OpenLLM），可以很好的屏蔽 LLM 的底层逻辑，实际使用的时候比直接手写 Transformer 要方便太多了。而且集成了远程 API 的访问，同时可以和 LangChain 结合，可以作为生产服务的 LLM 后端使用。</p></blockquote><p>文章内容：</p><ul><li>介绍</li><li>安装、运行、本地和远程接口调用和部署（ChatGLM2为例）</li><li>如何运行 Llama 2</li><li>部署说明</li></ul><h2 id="介绍" tabindex="-1">介绍 <a class="header-anchor" href="#介绍" aria-label="Permalink to &quot;介绍&quot;">​</a></h2><p>一句话介绍：OpenLLM 是可用于在生产中操作大型语言模型 (LLM) 的开放平台。 使用 OpenLLM 可以轻松微调、服务、部署和监控任何大语言模型。</p><p>也就是说，借助 OpenLLM，您可以使用任何开源大语言模型运行推理，部署到云端或本地，并构建强大的 AI 应用程序。</p><p>OpenLLM 有如下特点：</p><p>🚂 <strong>最先进的LLM</strong>：内置支持广泛的开源LLM和模型运行时，包括<code>Llama 2</code>，<code>StableLM</code>，<code>Falcon</code>，<code>Dolly</code>，<code>Flan-T5</code>，<code>ChatGLM</code>，<code>StarCoder</code>等。</p><p>🔥<strong>灵活的 API</strong>：使用一个命令通过 <code>RESTful API</code> 或 <code>gRPC</code> 为 LLM 提供服务，通过 WebUI、CLI、我们的 Python/Javascript 客户端或任何 HTTP 客户端进行查询。</p><p>⛓️ <strong>自由构建</strong>：对 <code>LangChain</code>、<code>BentoML</code> 和 <code>Hugging Face</code> 的一流支持，使您可以通过将 LLM 与其他模型和服务组合来轻松创建自己的 AI 应用程序。</p><p>🎯 <strong>简化部署</strong>：自动生成 LLM 服务器 Docker 映像或通过 ☁️ BentoCloud部署为无服务器端点。</p><p>🤖️ 生成自己的 LLM：微调任何法学硕士以满足你的需求 LLM.tuning()</p><h2 id="如何使用" tabindex="-1">如何使用 <a class="header-anchor" href="#如何使用" aria-label="Permalink to &quot;如何使用&quot;">​</a></h2><p>首先还是 pip 安装：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> openllm</span></span></code></pre></div><p>如果想使用清华大学的 ChatGLM 或者 ChatGLM 2，需要安装 OpenLLM 的变种：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &quot;openllm[chatglm]&quot;</span></span></code></pre></div><p>然后验证一下安装成果：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">$</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> openllm</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> -h</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">Usage:</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> openllm</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> [OPTIONS] COMMAND [ARGS]...</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">   ██████╗</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ██████╗</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ███████╗███╗</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">   ██╗██╗</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">     ██╗</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">     ███╗</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">   ███╗</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">  ██╔═══██╗██╔══██╗██╔════╝████╗</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  ██║██║</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">     ██║</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">     ████╗</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ████║</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">  ██║</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">   ██║██████╔╝█████╗</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  ██╔██╗</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ██║██║</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">     ██║</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">     ██╔████╔██║</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">  ██║</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">   ██║██╔═══╝</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ██╔══╝</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  ██║╚██╗██║██║</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">     ██║</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">     ██║╚██╔╝██║</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">  ╚██████╔╝██║</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">     ███████╗██║</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ╚████║███████╗███████╗██║</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ╚═╝</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ██║</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">   ╚═════╝</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ╚═╝</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">     ╚══════╝╚═╝</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  ╚═══╝╚══════╝╚══════╝╚═╝</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">     ╚═╝</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">  An</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> open</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> platform</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> for</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> operating</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> large</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> language</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> models</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> in</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> production.</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">  Fine-tune,</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> serve,</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> deploy,</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> and</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> monitor</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> any</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> LLMs</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> with</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> ease.</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">...</span></span></code></pre></div><h2 id="启动llm服务器" tabindex="-1">启动LLM服务器 <a class="header-anchor" href="#启动llm服务器" aria-label="Permalink to &quot;启动LLM服务器&quot;">​</a></h2><p>要启动 LLM 服务器，要使用 <code>openllm start</code>. 例如，要启动 <code>OPT</code> 服务器：：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">openllm</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> start</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> opt</span></span></code></pre></div><p>我们用 ChatGLM2 测试，指定模型型号的端口：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">openllm</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  start</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  chatglm</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --model-id=THUDM/chatglm2-6b</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --port=3600</span></span></code></pre></div><p>如果机器硬件不够，可以指定量化：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">openllm</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  start</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  chatglm</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --model-id=THUDM/chatglm2-6b</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --quantize</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> int8</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --port=3600</span></span></code></pre></div><p>启动成功，访问 <code>http://127.0.0.1:3600</code> 可以看到：</p><p><img src="`+i+`" alt=""></p><p>ChatGLM 架构 支持的模型 ID 如下：</p><ul><li><code>thudm/chatglm-6b</code></li><li><code>thudm/chatglm-6b-int8</code></li><li><code>thudm/chatglm-6b-int4</code></li><li><code>thudm/chatglm2-6b</code></li><li><code>thudm/chatglm2-6b-int4</code></li></ul><h3 id="embedding" tabindex="-1">Embedding <a class="header-anchor" href="#embedding" aria-label="Permalink to &quot;Embedding&quot;">​</a></h3><p>可以 <code>openllm embed</code> 命令进行量化：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> openllm embed --endpoint http://127.0.0.1:3600 </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;I like to eat apples&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> -o json</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">{</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">  &quot;embeddings&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">:</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> [</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">    0.006569798570126295,</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">    -0.031249752268195152,</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">    -0.008072729222476482,</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">    0.00847396720200777,</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">    -0.005293501541018486,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">    ...</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">&lt;many </span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">embedding</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">s</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">&gt;</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">...</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">    -0.002078012563288212,</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">    -0.00676426338031888,</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">    -0.002022686880081892</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">  ],</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">  &quot;num_tokens&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;">:</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> 9</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">}</span></span></code></pre></div><p>或者使用 Python SDK 调用：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> openllm</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">client </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> openllm.client.HTTPClient(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;http://127.0.0.1:3600&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">client.embed(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;I like to eat apples&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span></code></pre></div><h2 id="集成-langchain" tabindex="-1">集成 LangChain <a class="header-anchor" href="#集成-langchain" aria-label="Permalink to &quot;集成 LangChain&quot;">​</a></h2><h3 id="安装" tabindex="-1">安装 <a class="header-anchor" href="#安装" aria-label="Permalink to &quot;安装&quot;">​</a></h3><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> langchain[llms]</span></span></code></pre></div><h3 id="本地模型" tabindex="-1">本地模型 <a class="header-anchor" href="#本地模型" aria-label="Permalink to &quot;本地模型&quot;">​</a></h3><p>可以使用 OpenLLM 本地直接启动模型调用（不需要刚刚的<code>openllm start</code>）：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> langchain.llms </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> OpenLLM</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">llm </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> OpenLLM(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">model_name</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;llama&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">model_id</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;meta-llama/Llama-2-7b-hf&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">llm(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;What is the difference between a duck and a goose? And why there are so many Goose in Canada?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span></code></pre></div><h3 id="远程交互" tabindex="-1">远程交互 <a class="header-anchor" href="#远程交互" aria-label="Permalink to &quot;远程交互&quot;">​</a></h3><p>当然也支持远程调用，这样你就可以在服务器上启动 <code>openllm start</code>，然后在任意客户端访问。</p><p>使用 <code>langchain.llms.OpenLLM</code> 类来与远程 OpenLLM 服务器交互，通过指定其 URL 来连接：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> langchain.llms </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> OpenLLM</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">llm </span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;"> OpenLLM(</span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">server_url</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;http://xx.xx.xx.1:3600&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#F69D50;">server_type</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&#39;grpc&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">llm(</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">&quot;What is the difference between a duck and a goose? And why there are so many Goose in Canada?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">)</span></span></code></pre></div><h3 id="其他集成" tabindex="-1">其他集成 <a class="header-anchor" href="#其他集成" aria-label="Permalink to &quot;其他集成&quot;">​</a></h3><p>还可以和 Transformers 代理集成，用的不多，我就不介绍了。</p><h3 id="部署" tabindex="-1">部署 <a class="header-anchor" href="#部署" aria-label="Permalink to &quot;部署&quot;">​</a></h3><p>可以通过构建 Bento 或者 容器运行。</p><p>使用如下命令构建容器：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">bentoml</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> containerize</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;"> &lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">name:versio</span><span style="--shiki-light:#24292E;--shiki-dark:#ADBAC7;">n</span><span style="--shiki-light:#D73A49;--shiki-dark:#F47067;">&gt;</span></span></code></pre></div><p>当然最方便的是使用 BentoCloud 运行，如果使用了 BentoCloud ，应该很熟悉部署操作的，不再赘述了。</p><h2 id="使用-llama-2" tabindex="-1">使用 Llama 2 <a class="header-anchor" href="#使用-llama-2" aria-label="Permalink to &quot;使用 Llama 2&quot;">​</a></h2><h3 id="安装-llama-变体" tabindex="-1">安装 llama 变体 <a class="header-anchor" href="#安装-llama-变体" aria-label="Permalink to &quot;安装 llama 变体&quot;">​</a></h3><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> &quot;openllm[llama]&quot;</span></span></code></pre></div><h3 id="启动-llama-2" tabindex="-1">启动 Llama 2 <a class="header-anchor" href="#启动-llama-2" aria-label="Permalink to &quot;启动 Llama 2&quot;">​</a></h3><p>OpenLLM 支持的模型 ID 如下：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> meta-llama/Llama-2-70b-chat-hf</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> meta-llama/Llama-2-13b-chat-hf</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> meta-llama/Llama-2-7b-chat-hf</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> meta-llama/Llama-2-70b-hf</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> meta-llama/Llama-2-13b-hf</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> meta-llama/Llama-2-7b-hf</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> NousResearch/llama-2-70b-chat-hf</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> NousResearch/llama-2-13b-chat-hf</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> NousResearch/llama-2-7b-chat-hf</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> NousResearch/llama-2-70b-hf</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> NousResearch/llama-2-13b-hf</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> NousResearch/llama-2-7b-hf</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> openlm-research/open_llama_7b_v2</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> openlm-research/open_llama_3b_v2</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> openlm-research/open_llama_13b</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> huggyllama/llama-65b</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> huggyllama/llama-30b</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> huggyllama/llama-13b</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">-</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> huggyllama/llama-7b</span></span></code></pre></div><p>所以你可以启动：</p><div class="language-shell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">shell</span><pre class="shiki shiki-themes github-light github-dark-dimmed vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#F69D50;">openllm</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;">  start</span><span style="--shiki-light:#032F62;--shiki-dark:#96D0FF;"> llama</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --model-id=meta-llama/Llama-2-7b-hf</span><span style="--shiki-light:#005CC5;--shiki-dark:#6CB6FF;"> --port=3600</span></span></code></pre></div><p>我之前就下载过模型，所以启动很快，如果第一次用，可能要下载模型下载很久。</p><h2 id="最后" tabindex="-1">最后 <a class="header-anchor" href="#最后" aria-label="Permalink to &quot;最后&quot;">​</a></h2><p>如果是个人使用，目前为止还是 ChatGPT、Bard、Claude 这样的商业服务效果更好， 但是你要确定绑定开源生态，或者使用自己的模型做应用，那么基于 OpenLLM 开发有很大的灵活性。</p><hr><div style="text-align:center;color:#00000099;font-size:14px;">END</div>`,65)]))}const y=a(t,[["render",e]]);export{c as __pageData,y as default};
