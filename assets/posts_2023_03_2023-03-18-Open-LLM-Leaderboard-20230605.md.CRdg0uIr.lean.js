import{_ as o,a as e,af as t,o as n}from"./chunks/framework.C87LdZyP.js";const r="/assets/789074863861583.CWzfSkh3.png",u=JSON.parse('{"title":"HF开源大型语言模型（LLM）排行榜 2023-06-05","description":"","frontmatter":{"title":"HF开源大型语言模型（LLM）排行榜 2023-06-05","date":"2023-03-18T00:00:00.000Z","tags":["ai","ml"],"category":["AI"]},"headers":[],"relativePath":"posts/2023/03/2023-03-18-Open-LLM-Leaderboard-20230605.md","filePath":"posts/2023/03/2023-03-18-Open-LLM-Leaderboard-20230605.md","lastUpdated":1718173059000}'),l={name:"posts/2023/03/2023-03-18-Open-LLM-Leaderboard-20230605.md"};function s(i,a,c,p,h,d){return n(),e("div",null,a[0]||(a[0]=[t('<blockquote><p>随着大量的大型语言模型 (LLM) 和聊天机器人一周接一周地发布， 并且常常夸大其性能，很难过滤掉开源社区正在取得的真正进步以及哪种模型是当前最先进的技术。 🤗 Open LLM Leaderboard 旨在跟踪、排名和评估发布的 LLM 和聊天机器人。</p></blockquote><p>🤗 该排行榜的一个关键优势在于，社区中的任何人都可以提交模型以在 🤗 GP​​U 集群上进行自动评估， 只要它是一个在集线器上具有权重的 🤗 Transformers 模型。 我们还支持对非商业许可模型（例如 LLaMa）的具有增量权重的模型进行评估。</p><h2 id="评估标准" tabindex="-1">评估标准 <a class="header-anchor" href="#评估标准" aria-label="Permalink to &quot;评估标准&quot;">​</a></h2><p>📈 我们在 Eleuther AI 语言模型评估工具的 4 个关键基准上评估模型， 这是一个统一的框架，用于在大量不同的评估任务上测试生成语言模型：</p><ul><li><strong>AI2 推理挑战</strong>（25 次）- 一组小学科学问题。</li><li><strong>HellaSwag</strong>（10 次）- 常识推理测试，这对人类来说很容易 (~95%) 但对 SOTA 模型来说具有挑战性。</li><li><strong>MMLU</strong> (5-shot) - 衡量文本模型多任务准确性的测试。该测试涵盖初等数学、美国历史、计算机科学、法律等 57 项任务。</li><li><strong>TruthfulQA</strong> (0-shot) - 衡量语言模型在生成问题答案时是否真实的基准。</li></ul><p>我们选择这些基准测试是因为它们在 0-shot 和 few-shot 设置中测试了各个领域的各种推理和常识。</p><h2 id="完整排名" tabindex="-1">完整排名 <a class="header-anchor" href="#完整排名" aria-label="Permalink to &quot;完整排名&quot;">​</a></h2><p>先看一下完整截图：</p><p><img src="'+r+'" alt=""></p><h2 id="no01-falcon-40b-instruct" tabindex="-1">No01: Falcon-40B-Instruct <a class="header-anchor" href="#no01-falcon-40b-instruct" aria-label="Permalink to &quot;No01: Falcon-40B-Instruct&quot;">​</a></h2><p><strong>Falcon-40B-Instruct</strong> 是 TII 基于 <strong>Falcon-40B</strong> 构建的 <strong>40B</strong> 参数因果解码器专用模型， 并在 <strong>Baize</strong> 的混合上进行了微调。它在 Apache 2.0 许可下可用。</p><h2 id="no02-30b-lazarus" tabindex="-1">No02: 30B-Lazarus <a class="header-anchor" href="#no02-30b-lazarus" aria-label="Permalink to &quot;No02: 30B-Lazarus&quot;">​</a></h2><p>该模型是 <strong>CalderaAI</strong> 在语言模型和模型合并上实验性使用 LoRA 的结果， 这些语言模型和模型合并不是它们预期的基础 HuggingFace 格式 LLaMA 模型。 期望的结果是附加地应用期望的特征，而不自相矛盾地淡化模型的有效行为。</p><h2 id="no03-falcon-40b" tabindex="-1">No03: Falcon-40B <a class="header-anchor" href="#no03-falcon-40b" aria-label="Permalink to &quot;No03: Falcon-40B&quot;">​</a></h2><p><strong>Falcon-40B</strong> 是一个 40B 参数的因果解码器模型，由TII构建， 并在RefinedWeb的 1,000B 标记上训练， 并使用精选语料库进行了增强。它在 Apache 2.0 许可下可用。</p><h2 id="no04-guanaco-33b-merged" tabindex="-1">No04: Guanaco-33B-merged <a class="header-anchor" href="#no04-guanaco-33b-merged" aria-label="Permalink to &quot;No04: Guanaco-33B-merged&quot;">​</a></h2><p>这个模型还没有创建模型卡。</p><h2 id="no05-llama-30b-supercot" tabindex="-1">No05: llama-30b-supercot <a class="header-anchor" href="#no05-llama-30b-supercot" aria-label="Permalink to &quot;No05: llama-30b-supercot&quot;">​</a></h2><p>huggyllama/llama-30b + kaiokendev/SuperCOT-LoRA的合并</p><p>Supercot 经过培训可以使用 langchain 提示。</p><p>作者使用 Oobabooga 模块加载模型的自定义 LLM 笔记本中本地加载：<a href="https://github.com/ausboss/Local-LLM-Langchain" target="_blank" rel="noreferrer">https://github.com/ausboss/Local-LLM-Langchain</a></p><h2 id="no06-llama-65b" tabindex="-1">No06: llama-65b <a class="header-anchor" href="#no06-llama-65b" aria-label="Permalink to &quot;No06: llama-65b&quot;">​</a></h2><p>Meta 开放的650 亿参数的大型语言模型。</p><h2 id="no07-gpt4-x-alpasta-30b" tabindex="-1">No07: GPT4-X-Alpasta-30b <a class="header-anchor" href="#no07-gpt4-x-alpasta-30b" aria-label="Permalink to &quot;No07: GPT4-X-Alpasta-30b&quot;">​</a></h2><p>保留其出色的能力的同时改进 Open Assistant 作为指令的性能的尝试。 合并包括Chansung 的 GPT4-Alpaca Lora和Open Assistant 的原生微调。</p><h2 id="no08-vicunlocked-alpaca-30b" tabindex="-1">No08: VicUnlocked-alpaca-30b <a class="header-anchor" href="#no08-vicunlocked-alpaca-30b" aria-label="Permalink to &quot;No08: VicUnlocked-alpaca-30b&quot;">​</a></h2><p>虽然这是在像 Vicuna 使用的清理过的 ShareGPT 数据集上训练的，但它是以羊驼格式训练的，</p><h2 id="no09-alpacino30b" tabindex="-1">No09: Alpacino30b <a class="header-anchor" href="#no09-alpacino30b" aria-label="Permalink to &quot;No09: Alpacino30b&quot;">​</a></h2><p>-Alpac(ino) 代表 Alpaca Integrated Narrative Optimization。</p><p>该模型是（Alpaca+（CoT+Storytelling））的三重模型融合， 全面提升了Alpaca的推理和故事写作能力。羊驼被选为此次合并的支柱，以确保羊驼的指令格式保持主导地位。</p><p>此模型已获得非商业许可。此版本包含修改后的 Llama30b 权重，并且与善意相称， 即那些下载和/或使用此模型的人在填写以下表格后已被 Meta AI 授予对原始 Llama 权重的明确访问权。</p><h2 id="no10-gpt4-x-alpacadente2-30b" tabindex="-1">No10: GPT4-x-AlpacaDente2-30b <a class="header-anchor" href="#no10-gpt4-x-alpacadente2-30b" aria-label="Permalink to &quot;No10: GPT4-x-AlpacaDente2-30b&quot;">​</a></h2><p>ChanSung 的Alpaca-LoRA-30B-elina与Open Assistant 的第二个 Finetune合并。</p><hr><div style="text-align:center;color:#00000099;font-size:14px;">END</div>',35)]))}const g=o(l,[["render",s]]);export{u as __pageData,g as default};
