import{_ as i,a as e,af as t,o}from"./chunks/framework.C87LdZyP.js";const s="/assets/2777205207000.CLoo2qSK.png",l="/assets/18042200076916.DSJp0158.png",r="/assets/2777205207000.CLoo2qSK.png",n="/assets/20254617898041.Bh6EwUI_.png",d="/assets/20432814515291.Diw5rVBu.png",f="/assets/2777205207000.CLoo2qSK.png",D=JSON.parse('{"title":"Stable Video Diffusion：开源的AI视频生成大模型","description":"","frontmatter":{"title":"Stable Video Diffusion：开源的AI视频生成大模型","date":"2024-02-28T00:00:00.000Z","tags":["ai","llm","google"],"category":["ai"]},"headers":[],"relativePath":"posts/2024/02/2024-02-28-stable-video-diffusion.md","filePath":"posts/2024/02/2024-02-28-stable-video-diffusion.md","lastUpdated":1718173059000}'),p={name:"posts/2024/02/2024-02-28-stable-video-diffusion.md"};function b(u,a,c,h,m,S){return o(),e("div",null,a[0]||(a[0]=[t('<div class="admonition abstract"><p class="admonition-title">abstract</p><p>来看看文生图模型 Stable Diffusio -所属公司 stability.ai 在AI视频生成上发展的如何。 并探讨下新模型 SVD 和 传统 SD 生成视频的区别。</p></div><p>先放一个视频，文末还有更多示例。</p><p><img src="'+s+'" alt=""></p><p>说到文生图，大家可能想到最牛的是 <code>Midjourney</code>，说到开源的文生图，好像大家只能 想到 <code>Stable Diffusion</code>，而网上各路大神训练的模型几乎都是基于 StableDiffusion。</p><p><img src="'+l+'" alt="Stable Diffusio  -各式各样的模型"></p><p>Stable Diffusio -则是属于 Stability.AI 公司的， 前段时间还发布了 SD3。作为文生图世界的王者不说，我们今天聊一聊 它在文生视频上的进展。</p><p><img src="'+r+'" alt=""></p><h2 id="stable-video-diffusion" tabindex="-1">Stable Video Diffusion <a class="header-anchor" href="#stable-video-diffusion" aria-label="Permalink to &quot;Stable Video Diffusion&quot;">​</a></h2><p>stability.ai 公司的文生视频产品叫做 Stable Video Diffusion（简称 SVD）， 可以参考他的<a href="https://stability.ai/news/stable-video-diffusion-open-ai-video-model" target="_blank" rel="noreferrer">官网的介绍</a>。</p><blockquote><p>Stable Video Diffusio -23 年 11 月份就已经发布了哦。</p></blockquote><h2 id="stable-video-diffusion-img2vid-xt" tabindex="-1">stable-video-diffusion-img2vid-xt <a class="header-anchor" href="#stable-video-diffusion-img2vid-xt" aria-label="Permalink to &quot;stable-video-diffusion-img2vid-xt&quot;">​</a></h2><p>这是一个图片到视频的模型，已经开源，可以在 HuggingFace 上下载 <a href="https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt" target="_blank" rel="noreferrer">stable-video-diffusion-img2vid-xt</a>。</p><p>官方还吧模型和流行的AI视频工具 Runway Gen2 和 PikaLabs 做了对比。</p><p><img src="'+n+'" alt=""></p><p>在 25帧的视频上，视频质量是优于 Gen2 和 PikaLabs，在 15帧的视频上，和 Gen2 相当， 优于 PikaLabs。</p><p>更详细的信息可以阅读 <a href="https://static1.squarespace.com/static/6213c340453c3f502425776e/t/655ce779b9d47d342a93c890/1700587395994/stable_video_diffusion.pdf" target="_blank" rel="noreferrer">Stable Video Diffusio -论文</a>。</p><p><img src="'+d+'" alt=""></p><h2 id="svd-的限制" tabindex="-1">SVD 的限制 <a class="header-anchor" href="#svd-的限制" aria-label="Permalink to &quot;SVD 的限制&quot;">​</a></h2><ul><li>只能生成 2-5 秒的视频</li><li>最多 30 帧</li><li>无法控制文字</li></ul><h2 id="和-sd-插件生成视频的区别" tabindex="-1">和 SD 插件生成视频的区别 <a class="header-anchor" href="#和-sd-插件生成视频的区别" aria-label="Permalink to &quot;和 SD 插件生成视频的区别&quot;">​</a></h2><p>在 SVD 出现之前，我们其实已经看到过很多AI制作的视频，他们大多的制作方式都是 生成一张张的图片，然后一帧帧的拼接起来。</p><p>下面看看，SVD 这个方案和传统的 SD + 插件方式生成视频的不同。</p><h3 id="生成方式不同" tabindex="-1">生成方式不同 <a class="header-anchor" href="#生成方式不同" aria-label="Permalink to &quot;生成方式不同&quot;">​</a></h3><p>Stable Video Diffusio -模型是<strong>直接生成视频</strong>， 而Stable Diffusion模型是生成一张张图片，然后再将这些图片合成视频。</p><h3 id="模型结构不同" tabindex="-1">模型结构不同 <a class="header-anchor" href="#模型结构不同" aria-label="Permalink to &quot;模型结构不同&quot;">​</a></h3><p>Stable Video Diffusion模型是一个多阶段训练的模型， 包括图像预训练、视频预训练和高质量视频微调。 而Stable Diffusion模型是一个单阶段训练的模型。</p><h3 id="生成效果不同" tabindex="-1">生成效果不同 <a class="header-anchor" href="#生成效果不同" aria-label="Permalink to &quot;生成效果不同&quot;">​</a></h3><p>Stable Video Diffusion模型生成的视频通常具有<strong>更高的质量和流畅性</strong>， 而Stable Diffusion模型生成的视频可能会出现<strong>卡顿、闪烁</strong>等问题。</p><h2 id="svd-效果" tabindex="-1">SVD 效果 <a class="header-anchor" href="#svd-效果" aria-label="Permalink to &quot;SVD 效果&quot;">​</a></h2><p>最后我们看一下 Stable Video Diffusio -的视频效果。</p><p><img src="'+f+'" alt=""></p>',31)]))}const v=i(p,[["render",b]]);export{D as __pageData,v as default};
